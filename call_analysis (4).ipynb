{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68707fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.11/site-packages (2.3.5)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.11/site-packages (3.0.0)\n",
      "Requirement already satisfied: matplotlib in /opt/homebrew/lib/python3.11/site-packages (3.10.8)\n",
      "Requirement already satisfied: librosa in /opt/homebrew/lib/python3.11/site-packages (0.11.0)\n",
      "Requirement already satisfied: soundfile in /opt/homebrew/lib/python3.11/site-packages (0.13.1)\n",
      "Requirement already satisfied: ipython in /Users/vikaskhare/Library/Python/3.11/lib/python/site-packages (9.9.0)\n",
      "Requirement already satisfied: python-dotenv in /opt/homebrew/lib/python3.11/site-packages (1.2.1)\n",
      "Requirement already satisfied: openai in /opt/homebrew/lib/python3.11/site-packages (2.16.0)\n",
      "Requirement already satisfied: google-generativeai in /opt/homebrew/lib/python3.11/site-packages (0.8.6)\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/lib/python3.11/site-packages (1.8.0)\n",
      "Requirement already satisfied: scipy in /opt/homebrew/lib/python3.11/site-packages (1.17.0)\n",
      "Requirement already satisfied: noisereduce in /opt/homebrew/lib/python3.11/site-packages (3.0.3)\n",
      "Requirement already satisfied: pydub in /opt/homebrew/lib/python3.11/site-packages (0.25.1)\n",
      "Requirement already satisfied: SpeechRecognition in /opt/homebrew/lib/python3.11/site-packages (3.14.5)\n",
      "Requirement already satisfied: langid in /opt/homebrew/lib/python3.11/site-packages (1.1.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/vikaskhare/Library/Python/3.11/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/vikaskhare/Library/Python/3.11/lib/python/site-packages (from matplotlib) (26.0)\n",
      "Requirement already satisfied: pillow>=8 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (3.3.2)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /opt/homebrew/lib/python3.11/site-packages (from librosa) (3.1.0)\n",
      "Requirement already satisfied: numba>=0.51.0 in /opt/homebrew/lib/python3.11/site-packages (from librosa) (0.63.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /opt/homebrew/lib/python3.11/site-packages (from librosa) (1.5.3)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /Users/vikaskhare/Library/Python/3.11/lib/python/site-packages (from librosa) (5.2.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /opt/homebrew/lib/python3.11/site-packages (from librosa) (1.9.0)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /opt/homebrew/lib/python3.11/site-packages (from librosa) (1.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in /Users/vikaskhare/Library/Python/3.11/lib/python/site-packages (from librosa) (4.15.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /opt/homebrew/lib/python3.11/site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /opt/homebrew/lib/python3.11/site-packages (from librosa) (1.1.2)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/homebrew/lib/python3.11/site-packages (from soundfile) (2.0.0)\n",
      "Requirement already satisfied: ipython-pygments-lexers>=1.0.0 in /Users/vikaskhare/Library/Python/3.11/lib/python/site-packages (from ipython) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.18.1 in /Users/vikaskhare/Library/Python/3.11/lib/python/site-packages (from ipython) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1.5 in /Users/vikaskhare/Library/Python/3.11/lib/python/site-packages (from ipython) (0.2.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/vikaskhare/Library/Python/3.11/lib/python/site-packages (from ipython) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /Users/vikaskhare/Library/Python/3.11/lib/python/site-packages (from ipython) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.11.0 in /Users/vikaskhare/Library/Python/3.11/lib/python/site-packages (from ipython) (2.19.2)\n",
      "Requirement already satisfied: stack_data>=0.6.0 in /Users/vikaskhare/Library/Python/3.11/lib/python/site-packages (from ipython) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /Users/vikaskhare/Library/Python/3.11/lib/python/site-packages (from ipython) (5.14.3)\n",
      "Requirement already satisfied: wcwidth in /Users/vikaskhare/Library/Python/3.11/lib/python/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython) (0.5.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/homebrew/lib/python3.11/site-packages (from openai) (4.12.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/homebrew/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/homebrew/lib/python3.11/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /opt/homebrew/lib/python3.11/site-packages (from openai) (0.13.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/homebrew/lib/python3.11/site-packages (from openai) (2.12.5)\n",
      "Requirement already satisfied: sniffio in /opt/homebrew/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/homebrew/lib/python3.11/site-packages (from openai) (4.67.2)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/homebrew/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/homebrew/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/homebrew/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/homebrew/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/homebrew/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /opt/homebrew/lib/python3.11/site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in /opt/homebrew/lib/python3.11/site-packages (from google-generativeai) (2.29.0)\n",
      "Requirement already satisfied: google-api-python-client in /opt/homebrew/lib/python3.11/site-packages (from google-generativeai) (2.189.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in /opt/homebrew/lib/python3.11/site-packages (from google-generativeai) (2.49.0.dev0)\n",
      "Requirement already satisfied: protobuf in /opt/homebrew/lib/python3.11/site-packages (from google-generativeai) (5.29.6)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /opt/homebrew/lib/python3.11/site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.27.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /opt/homebrew/lib/python3.11/site-packages (from google-api-core->google-generativeai) (1.72.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /opt/homebrew/lib/python3.11/site-packages (from google-api-core->google-generativeai) (2.32.5)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /opt/homebrew/lib/python3.11/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.76.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /opt/homebrew/lib/python3.11/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/homebrew/lib/python3.11/site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: cryptography>=38.0.3 in /opt/homebrew/lib/python3.11/site-packages (from google-auth>=2.15.0->google-generativeai) (46.0.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.6.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: pycparser in /opt/homebrew/lib/python3.11/site-packages (from cffi>=1.0->soundfile) (3.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /Users/vikaskhare/Library/Python/3.11/lib/python/site-packages (from jedi>=0.18.1->ipython) (0.8.5)\n",
      "Requirement already satisfied: llvmlite<0.47,>=0.46.0dev0 in /opt/homebrew/lib/python3.11/site-packages (from numba>=0.51.0->librosa) (0.46.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/vikaskhare/Library/Python/3.11/lib/python/site-packages (from pexpect>4.3->ipython) (0.7.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /Users/vikaskhare/Library/Python/3.11/lib/python/site-packages (from pooch>=1.1->librosa) (4.5.1)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /opt/homebrew/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/vikaskhare/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/vikaskhare/Library/Python/3.11/lib/python/site-packages (from stack_data>=0.6.0->ipython) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/vikaskhare/Library/Python/3.11/lib/python/site-packages (from stack_data>=0.6.0->ipython) (3.0.1)\n",
      "Requirement already satisfied: pure-eval in /Users/vikaskhare/Library/Python/3.11/lib/python/site-packages (from stack_data>=0.6.0->ipython) (0.2.3)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /opt/homebrew/lib/python3.11/site-packages (from google-api-python-client->google-generativeai) (0.31.2)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /opt/homebrew/lib/python3.11/site-packages (from google-api-python-client->google-generativeai) (0.3.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /opt/homebrew/lib/python3.11/site-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.11/bin/python3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Loading /Users/vikaskhare/Documents/sample_call.wav...\n",
      "Librosa failed to load /Users/vikaskhare/Documents/sample_call.wav: [Errno 21] Is a directory: '/Users/vikaskhare/Documents/sample_call.wav'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g_/zzf2f_250pb_mndpb1ygp24w0000gn/T/ipykernel_14230/1084651761.py:70: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(file_path, sr=None)\n",
      "/opt/homebrew/lib/python3.11/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install numpy pandas matplotlib librosa soundfile ipython python-dotenv openai google-generativeai scikit-learn scipy noisereduce pydub SpeechRecognition langid\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import json\n",
    "from IPython.display import Audio, display\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import google.generativeai as genai\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.signal import medfilt\n",
    "import glob\n",
    "import time\n",
    "import traceback \n",
    "import noisereduce as nr\n",
    "from pydub import AudioSegment\n",
    "import speech_recognition as speech_rec  # Renamed to avoid conflict\n",
    "import langid\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Configure API keys\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# Constants\n",
    "TARGET_SAMPLE_RATE = 16000\n",
    "OUTPUT_DIR = 'processed_audio'\n",
    "\n",
    "# region AudioProcessor with Noise Reduction\n",
    "class AudioProcessor:\n",
    "    def __init__(self, target_sr=TARGET_SAMPLE_RATE, output_dir=OUTPUT_DIR):\n",
    "        self.target_sr = target_sr\n",
    "        self.output_dir = output_dir  # Will be updated based on input file\n",
    "        \n",
    "    def reduce_noise(self, y, sr):\n",
    "        \"\"\"Apply noise reduction to clean the audio\"\"\"\n",
    "        print(\"Applying noise reduction...\")\n",
    "        # Estimate noise from a small portion of the audio (first 1 second)\n",
    "        noise_sample = y[:min(sr, len(y))]\n",
    "        # Apply noise reduction\n",
    "        reduced_noise = nr.reduce_noise(\n",
    "            y=y, \n",
    "            sr=sr,\n",
    "            stationary=True,\n",
    "            prop_decrease=0.75\n",
    "        )\n",
    "        print(\"Noise reduction complete\")\n",
    "        return reduced_noise\n",
    "        \n",
    "    def process_audio(self, file_path):\n",
    "        file_path = file_path.strip('\"')\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Error: File {file_path} not found.\")\n",
    "            return None, None, None\n",
    "        \n",
    "        # Set output directory to same folder as input\n",
    "        self.output_dir = os.path.dirname(file_path)\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        \n",
    "        filename = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        output_path = os.path.join(self.output_dir, f\"{filename}_16khz.wav\")\n",
    "\n",
    "        print(f\"Loading {file_path}...\")\n",
    "        try:\n",
    "            y, sr = librosa.load(file_path, sr=None)\n",
    "        except Exception as e:\n",
    "            print(f\"Librosa failed to load {file_path}: {e}\")\n",
    "            return None, None, None\n",
    "\n",
    "        print(f\"Original: Sample Rate = {sr} Hz, Duration = {librosa.get_duration(y=y, sr=sr):.2f} seconds\")\n",
    "        \n",
    "        # Apply noise reduction\n",
    "        y = self.reduce_noise(y, sr)\n",
    "        \n",
    "        if sr != self.target_sr:\n",
    "            print(f\"Resampling to {self.target_sr} Hz...\")\n",
    "            # Use faster resampling method for better performance\n",
    "            y = librosa.resample(y, orig_sr=sr, target_sr=self.target_sr, res_type='kaiser_fast')\n",
    "\n",
    "        sf.write(output_path, y, self.target_sr, subtype='PCM_16')\n",
    "        print(f\"Saved to {output_path}\")\n",
    "        return output_path, y, self.target_sr\n",
    "\n",
    "    def display_audio_info(self, file_path, y, sr):\n",
    "        if y is None or sr is None:\n",
    "            print(f\"Skipping visualization for {file_path} due to loading error.\")\n",
    "            return\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.plot(np.linspace(0, len(y)/sr, len(y)), y)\n",
    "        plt.title(f'Waveform: {os.path.basename(file_path)}')\n",
    "        plt.xlabel('Time (s)')\n",
    "        plt.ylabel('Amplitude')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(\"Audio playback:\")\n",
    "        display(Audio(y, rate=sr))\n",
    "# endregion\n",
    "\n",
    "# region FeatureExtractor\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, sample_rate=TARGET_SAMPLE_RATE):\n",
    "        self.sample_rate = sample_rate\n",
    "\n",
    "    def extract_all_features(self, audio_path, y, sr):\n",
    "        return {\n",
    "            'opensmile': self.extract_custom_opensmile(y, sr),\n",
    "            'praat': self.extract_praat_features(y, sr)\n",
    "        }\n",
    "\n",
    "    def extract_custom_opensmile(self, y, sr):\n",
    "        print(\"Extracting custom OpenSMILE-style features...\")\n",
    "        # Use larger hop_length for faster processing\n",
    "        hop_length = 512\n",
    "        pitch, voiced_flag, _ = librosa.pyin(y, fmin=75, fmax=400, sr=sr, hop_length=hop_length, fill_na=0.0)\n",
    "        pitch_valid = pitch[voiced_flag]\n",
    "        jitter = 0.01\n",
    "        if len(pitch_valid) > 1:\n",
    "            pitch_diffs = np.abs(np.diff(pitch_valid))\n",
    "            jitter = np.mean(pitch_diffs) / np.mean(pitch_valid) if np.mean(pitch_valid) > 0 else 0.01\n",
    "\n",
    "        rms = librosa.feature.rms(y=y, hop_length=hop_length)[0]\n",
    "        shimmer = 0.04\n",
    "        if len(rms) > 1:\n",
    "            rms_diffs = np.abs(np.diff(rms))\n",
    "            shimmer = np.mean(rms_diffs) / np.mean(rms) if np.mean(rms) > 0 else 0.04\n",
    "\n",
    "        features = {\n",
    "            'loudness': np.mean(librosa.feature.rms(y=y, hop_length=hop_length)),\n",
    "            'energy': np.sum(y**2) / len(y),\n",
    "            'pitch_mean': np.mean(pitch_valid) if np.any(voiced_flag) else 0,\n",
    "            'pitch_range': np.ptp(pitch_valid) if np.any(voiced_flag) else 0,\n",
    "            'jitter': jitter,\n",
    "            'shimmer': shimmer\n",
    "        }\n",
    "        return pd.DataFrame([features])\n",
    "\n",
    "    def extract_praat_features(self, y, sr):\n",
    "        print(\"Extracting Praat-like features...\")\n",
    "        # Use larger hop_length for faster processing\n",
    "        hop_length = 512\n",
    "        pitch, voiced_flag, voiced_probs = librosa.pyin(y, fmin=75, fmax=400, sr=sr, hop_length=hop_length, fill_na=0.0)\n",
    "        S = np.abs(librosa.stft(y, hop_length=hop_length))\n",
    "        spectral_flatness = librosa.feature.spectral_flatness(S=S)[0]\n",
    "        hnr_estimate = 20 * (1 - np.mean(spectral_flatness))\n",
    "        intensity = np.mean(librosa.feature.rms(y=y, hop_length=hop_length)[0]) * 100\n",
    "\n",
    "        onset_env = librosa.onset.onset_strength(y=y, sr=sr, hop_length=hop_length)\n",
    "        peaks = librosa.util.peak_pick(onset_env, pre_max=3, post_max=3, pre_avg=3, post_avg=3, delta=0.5, wait=0.5)\n",
    "\n",
    "        duration = len(y) / sr\n",
    "        speaking_rate = len(peaks) / duration if duration > 0 else 4.0\n",
    "        speaking_rate = max(3.0, min(5.5, speaking_rate))\n",
    "\n",
    "        features = {\n",
    "            'F0_mean': np.mean(pitch[voiced_flag]) if np.any(voiced_flag) else 0,\n",
    "            'F0_std': np.std(pitch[voiced_flag]) if np.any(voiced_flag) else 0,\n",
    "            'HNR': hnr_estimate,\n",
    "            'Intensity_mean': intensity,\n",
    "            'Speaking_rate': speaking_rate\n",
    "        }\n",
    "        return pd.DataFrame([features])\n",
    "# endregion\n",
    "\n",
    "# region VoiceAnalyzer\n",
    "class VoiceAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.sentiment_mapping = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "        self.politeness_mapping = {0: \"Impolite\", 1: \"Neutral\", 2: \"Polite\"}\n",
    "        self.empathy_mapping = {0: \"Low Empathy\", 1: \"Moderate Empathy\", 2: \"High Empathy\"}\n",
    "\n",
    "    def extract_indicators(self, features):\n",
    "        indicators = {}\n",
    "        if 'opensmile' in features:\n",
    "            osm = features['opensmile']\n",
    "            indicators.update({k: osm[k].values[0] for k in osm.columns})\n",
    "        if 'praat' in features:\n",
    "            praat = features['praat']\n",
    "            indicators.update({k.lower(): praat[k].values[0] for k in praat.columns})\n",
    "        return indicators\n",
    "\n",
    "    def analyze_voice(self, indicators):\n",
    "        results = {}\n",
    "        s_base, p_base, e_base = 5.0, 5.0, 5.0\n",
    "        e = indicators['energy']\n",
    "        pr = indicators['pitch_range']\n",
    "        j = indicators['jitter']\n",
    "        sr = indicators['speaking_rate']\n",
    "        i = indicators['intensity_mean']\n",
    "        f0 = indicators['f0_mean']\n",
    "        h = indicators['hnr']\n",
    "        sh = indicators['shimmer']\n",
    "\n",
    "        sentiment_score = s_base + min(2.0, e*10) + min(2.0, pr/50) + max(0, 1.0 - j*50) + (\n",
    "            2.0 if 4.0 <= sr <= 5.0 else 1.5 if 3.5 <= sr < 4.0 or 5.0 < sr <= 5.5 else 1.0)\n",
    "        sentiment_score = max(1.0, min(10.0, sentiment_score))\n",
    "        results['sentiment'] = {\n",
    "            'score': round(sentiment_score, 1),\n",
    "            'description': self._get_score_description(sentiment_score, \"sentiment\")\n",
    "        }\n",
    "\n",
    "        p_score = p_base + (\n",
    "            2.0 if pr < 50 else 1.5 if pr < 80 else 1.0 if pr < 120 else 0) + (\n",
    "            2.0 if i < 60 else 1.5 if i < 70 else 1.0 if i < 80 else 0) + (\n",
    "            2.0 if 3.8 <= sr <= 4.5 else 1.5 if 3.5 <= sr < 3.8 or 4.5 < sr <= 5.0 else 0.5) + min(1.0, h / 20)\n",
    "        p_score = max(1.0, min(10.0, p_score))\n",
    "        results['politeness'] = {\n",
    "            'score': round(p_score, 1),\n",
    "            'description': self._get_score_description(p_score, \"politeness\")\n",
    "        }\n",
    "\n",
    "        e_score = e_base + (\n",
    "            1.5 if 180 < f0 < 280 else 0) + (\n",
    "            1.0 if 30 < pr < 90 and 180 < f0 < 280 else 0) + (\n",
    "            1.5 if h > 15 else 1.0 if h > 10 else 0) + (\n",
    "            1.5 if 3.8 <= sr <= 4.5 else 1.0 if 3.5 <= sr < 3.8 or 4.5 < sr <= 5.0 else 0) + (\n",
    "            1.0 if 0.03 <= sh <= 0.06 else 0)\n",
    "        e_score = max(1.0, min(10.0, e_score))\n",
    "        results['empathy'] = {\n",
    "            'score': round(e_score, 1),\n",
    "            'description': self._get_score_description(e_score, \"empathy\")\n",
    "        }\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _get_score_description(self, score, attribute):\n",
    "        bins = {\n",
    "            \"sentiment\": [\"Very negative\", \"Negative\", \"Slightly negative\", \"Neutral\", \"Slightly positive\", \"Positive\", \"Very positive\"],\n",
    "            \"politeness\": [\"Impolite\", \"Direct/abrupt\", \"Somewhat direct\", \"Neutral\", \"Polite\", \"Very polite\", \"Extremely polite\"],\n",
    "            \"empathy\": [\"Very distant/cold\", \"Distant\", \"Somewhat distant\", \"Moderately empathetic\", \"Empathetic\", \"Very empathetic\", \"Highly empathetic\"]\n",
    "        }\n",
    "        idx = int(min(6, max(0, round(score // 1.5))))\n",
    "        return bins.get(attribute, [\"Unknown\"])[idx]\n",
    "\n",
    "    def analyze(self, features):\n",
    "        indicators = self.extract_indicators(features)\n",
    "        results = self.analyze_voice(indicators)\n",
    "        return results, indicators\n",
    "# endregion\n",
    "\n",
    "# region CallAnalyzer with Enhanced Speaker Diarization\n",
    "class CallAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.recognizer = speech_rec.Recognizer()  # Fixed: Use renamed module\n",
    "        \n",
    "    def process_audio(self, audio_path):\n",
    "        try:\n",
    "            y, sample_rate = librosa.load(audio_path, sr=None)  # Renamed to avoid conflict\n",
    "            return y, sample_rate\n",
    "        except Exception as e:\n",
    "            print(f\"Audio processing error: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def detect_language(self, audio_segment, sample_rate):\n",
    "        \"\"\"Detect language of an audio segment\"\"\"\n",
    "        try:\n",
    "            # Convert numpy array to bytes for speech recognition\n",
    "            audio_bytes = (audio_segment * 32767).astype(np.int16).tobytes()\n",
    "        \n",
    "            # Create AudioData with the correct parameters\n",
    "            sample_width = 2  # 16-bit audio = 2 bytes per sample\n",
    "            audio_data = speech_rec.AudioData(audio_bytes, sample_rate, sample_width)  # Fixed: Use renamed module\n",
    "        \n",
    "            # Try to recognize text\n",
    "            try:\n",
    "                text = self.recognizer.recognize_google(audio_data)\n",
    "                # Detect language from text\n",
    "                lang, _ = langid.classify(text)\n",
    "                return lang\n",
    "            except Exception as e:\n",
    "                print(f\"Speech recognition error: {e}\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"Language detection error: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "    def extract_segment_features(self, segment, sample_rate):\n",
    "        \"\"\"Extract features from a segment to help with speaker identification\"\"\"\n",
    "        if len(segment) < sample_rate * 0.5:  # Skip segments shorter than 0.5 seconds\n",
    "            return None\n",
    "            \n",
    "        features = {}\n",
    "        \n",
    "        # Extract pitch with larger hop_length for faster processing\n",
    "        try:\n",
    "            hop_length = 512\n",
    "            pitch, voiced_flag, _ = librosa.pyin(\n",
    "                segment, \n",
    "                fmin=75, \n",
    "                fmax=400, \n",
    "                sr=sample_rate, \n",
    "                hop_length=hop_length,\n",
    "                fill_na=0.0\n",
    "            )\n",
    "            features['pitch_mean'] = np.mean(pitch[voiced_flag]) if np.any(voiced_flag) else 0\n",
    "        except:\n",
    "            features['pitch_mean'] = 0\n",
    "            \n",
    "        # Extract energy\n",
    "        try:\n",
    "            rms = librosa.feature.rms(y=segment, hop_length=hop_length)[0]\n",
    "            features['energy_mean'] = np.mean(rms)\n",
    "        except:\n",
    "            features['energy_mean'] = 0\n",
    "            \n",
    "        # Extract spectral features\n",
    "        try:\n",
    "            mfcc = librosa.feature.mfcc(y=segment, sr=sample_rate, n_mfcc=13, hop_length=hop_length)\n",
    "            features['mfcc_mean'] = np.mean(mfcc, axis=1)\n",
    "        except:\n",
    "            features['mfcc_mean'] = np.zeros(13)\n",
    "            \n",
    "        return features\n",
    "\n",
    "    def diarize_speakers(self, y, sample_rate):\n",
    "        \"\"\"\n",
    "        Improved speaker diarization with better segmentation and speaker identification\n",
    "        \"\"\"\n",
    "        print(\"Performing enhanced speaker diarization...\")\n",
    "        \n",
    "        # Step 1: Extract more robust features for segmentation\n",
    "        # Combine MFCCs with spectral contrast for better speaker differentiation\n",
    "        hop_length = 512\n",
    "        n_mfcc = 13\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sample_rate, n_mfcc=n_mfcc, hop_length=hop_length).T\n",
    "        \n",
    "        # Add spectral contrast features for better voice characteristic capture\n",
    "        contrast = librosa.feature.spectral_contrast(y=y, sr=sample_rate, hop_length=hop_length).T\n",
    "        \n",
    "        # Combine features\n",
    "        features = np.hstack([mfcc, contrast[:, :3]])  # Use first 3 contrast bands\n",
    "        \n",
    "        # Step 2: Use K-means for initial clustering with better initialization\n",
    "        # Try multiple initializations to find the best clustering\n",
    "        best_inertia = float('inf')\n",
    "        best_labels = None\n",
    "        \n",
    "        for i in range(3):  # Try 3 different initializations\n",
    "            kmeans = KMeans(n_clusters=2, random_state=i, n_init=2, max_iter=100).fit(features)\n",
    "            if kmeans.inertia_ < best_inertia:\n",
    "                best_inertia = kmeans.inertia_\n",
    "                best_labels = kmeans.labels_\n",
    "        \n",
    "        # Apply median filter to smooth labels and remove noise\n",
    "        labels = medfilt(best_labels, kernel_size=15)  # Increased kernel size for better smoothing\n",
    "        \n",
    "        # Step 3: Find segments with minimum duration\n",
    "        segments = []\n",
    "        current_label = labels[0]\n",
    "        start = 0\n",
    "        min_segment_frames = int(0.5 * sample_rate / hop_length)  # Minimum 0.5 seconds\n",
    "        \n",
    "        for i, label in enumerate(labels):\n",
    "            if label != current_label:\n",
    "                # Only keep segments longer than minimum duration\n",
    "                if i - start >= min_segment_frames:\n",
    "                    segments.append({\n",
    "                        'start': start,\n",
    "                        'end': i,\n",
    "                        'label': current_label,\n",
    "                        'start_time': start * hop_length / sample_rate,\n",
    "                        'end_time': i * hop_length / sample_rate\n",
    "                    })\n",
    "                start = i\n",
    "                current_label = label\n",
    "                \n",
    "        # Add the last segment\n",
    "        if len(labels) - start >= min_segment_frames:\n",
    "            segments.append({\n",
    "                'start': start,\n",
    "                'end': len(labels),\n",
    "                'label': current_label,\n",
    "                'start_time': start * hop_length / sample_rate,\n",
    "                'end_time': len(labels) * hop_length / sample_rate\n",
    "            })\n",
    "        \n",
    "        # Step 4: Extract audio for each segment and compute features\n",
    "        for segment in segments:\n",
    "            segment_start_sample = int(segment['start_time'] * sample_rate)\n",
    "            segment_end_sample = int(segment['end_time'] * sample_rate)\n",
    "            segment['audio'] = y[segment_start_sample:segment_end_sample]\n",
    "            \n",
    "            # Extract additional features for each segment\n",
    "            segment['features'] = self.extract_segment_features(segment['audio'], sample_rate)\n",
    "            \n",
    "            # Only detect language for longer segments\n",
    "            if len(segment['audio']) > sample_rate * 2:  # Only for segments > 2 seconds\n",
    "                segment['language'] = self.detect_language(segment['audio'], sample_rate)\n",
    "            else:\n",
    "                segment['language'] = None\n",
    "        \n",
    "        # Step 5: Improved speaker identification\n",
    "        # Analyze all segments to determine speaker characteristics\n",
    "        speaker_0_segments = [s for s in segments if s['label'] == 0 and s['features'] is not None]\n",
    "        speaker_1_segments = [s for s in segments if s['label'] == 1 and s['features'] is not None]\n",
    "        \n",
    "        # Skip analysis if we don't have enough segments for either speaker\n",
    "        if len(speaker_0_segments) < 2 or len(speaker_1_segments) < 2:\n",
    "            print(\"Not enough segments for reliable speaker identification. Using default assignment.\")\n",
    "            agent_label = 0  # Default: first speaker is agent\n",
    "            customer_label = 1\n",
    "        else:\n",
    "            # Extract key features for each speaker\n",
    "            speaker_0_pitch = np.mean([s['features']['pitch_mean'] for s in speaker_0_segments])\n",
    "            speaker_1_pitch = np.mean([s['features']['pitch_mean'] for s in speaker_1_segments])\n",
    "            \n",
    "            speaker_0_energy = np.mean([s['features']['energy_mean'] for s in speaker_0_segments])\n",
    "            speaker_1_energy = np.mean([s['features']['energy_mean'] for s in speaker_1_segments])\n",
    "            \n",
    "            # Calculate speaking time for each speaker\n",
    "            speaker_0_duration = sum([s['end_time'] - s['start_time'] for s in speaker_0_segments])\n",
    "            speaker_1_duration = sum([s['end_time'] - s['start_time'] for s in speaker_1_segments])\n",
    "            \n",
    "            # Check for language detection results\n",
    "            speaker_0_languages = [s['language'] for s in speaker_0_segments if s.get('language')]\n",
    "            speaker_1_languages = [s['language'] for s in speaker_1_segments if s.get('language')]\n",
    "            \n",
    "            # Determine which speaker is likely the agent based on multiple factors\n",
    "            agent_score_0 = 0\n",
    "            agent_score_1 = 0\n",
    "            \n",
    "            # 1. Agents typically have more consistent pitch (lower is better)\n",
    "            pitch_std_0 = np.std([s['features']['pitch_mean'] for s in speaker_0_segments])\n",
    "            pitch_std_1 = np.std([s['features']['pitch_mean'] for s in speaker_1_segments])\n",
    "            \n",
    "            if pitch_std_0 < pitch_std_1:\n",
    "                agent_score_0 += 1\n",
    "            else:\n",
    "                agent_score_1 += 1\n",
    "                \n",
    "            # 2. Agents typically speak more in customer service calls\n",
    "            if speaker_0_duration > speaker_1_duration:\n",
    "                agent_score_0 += 1\n",
    "            else:\n",
    "                agent_score_1 += 1\n",
    "                \n",
    "            # 3. Agents typically have moderate energy levels (not too high, not too low)\n",
    "            # Normalize energy to 0-1 range for comparison\n",
    "            max_energy = max(speaker_0_energy, speaker_1_energy)\n",
    "            if max_energy > 0:\n",
    "                norm_energy_0 = speaker_0_energy / max_energy\n",
    "                norm_energy_1 = speaker_1_energy / max_energy\n",
    "                \n",
    "                # Ideal agent energy is around 0.6-0.8 of max\n",
    "                if abs(norm_energy_0 - 0.7) < abs(norm_energy_1 - 0.7):\n",
    "                    agent_score_0 += 1\n",
    "                else:\n",
    "                    agent_score_1 += 1\n",
    "            \n",
    "            # 4. Check if non-English language is detected (customer more likely to speak non-English)\n",
    "            if speaker_0_languages and speaker_1_languages:\n",
    "                non_english_0 = sum(1 for lang in speaker_0_languages if lang != 'en')\n",
    "                non_english_1 = sum(1 for lang in speaker_1_languages if lang != 'en')\n",
    "                \n",
    "                if non_english_0 > non_english_1:\n",
    "                    # More non-English in speaker 0, so speaker 1 is more likely the agent\n",
    "                    agent_score_1 += 2  # Give this a higher weight\n",
    "                elif non_english_1 > non_english_0:\n",
    "                    # More non-English in speaker 1, so speaker 0 is more likely the agent\n",
    "                    agent_score_0 += 2  # Give this a higher weight\n",
    "            \n",
    "            # 5. Check who speaks first (agents typically speak first in calls)\n",
    "            if segments and len(segments) > 0:\n",
    "                first_speaker = segments[0]['label']\n",
    "                if first_speaker == 0:\n",
    "                    agent_score_0 += 1\n",
    "                else:\n",
    "                    agent_score_1 += 1\n",
    "            \n",
    "            # Determine agent based on scores\n",
    "            if agent_score_0 >= agent_score_1:\n",
    "                agent_label = 0\n",
    "                customer_label = 1\n",
    "            else:\n",
    "                agent_label = 1\n",
    "                customer_label = 0\n",
    "                \n",
    "            print(f\"Speaker identification scores - Speaker 0: {agent_score_0}, Speaker 1: {agent_score_1}\")\n",
    "            print(f\"Identified agent as Speaker {agent_label}, customer as Speaker {customer_label}\")\n",
    "        \n",
    "        # Step 6: Group segments by speaker with improved accuracy\n",
    "        agent_audio = []\n",
    "        customer_audio = []\n",
    "        \n",
    "        for segment in segments:\n",
    "            if segment['label'] == agent_label:\n",
    "                agent_audio.append(segment['audio'])\n",
    "            else:\n",
    "                customer_audio.append(segment['audio'])\n",
    "        \n",
    "        # Combine segments for each speaker\n",
    "        agent_audio = np.concatenate(agent_audio) if agent_audio else np.array([])\n",
    "        customer_audio = np.concatenate(customer_audio) if customer_audio else np.array([])\n",
    "        \n",
    "        print(f\"Diarization complete. Agent segments: {len(agent_audio)}, Customer segments: {len(customer_audio)}\")\n",
    "        return {\n",
    "            'agent': agent_audio,\n",
    "            'customer': customer_audio\n",
    "        }\n",
    "        \n",
    "    def save_speaker_audio(self, speakers_dict, sample_rate, original_file_path):\n",
    "        \"\"\"Save separate audio files for each speaker\"\"\"\n",
    "        try:\n",
    "            # Create output directory based on original file name\n",
    "            base_name = os.path.basename(original_file_path)\n",
    "            file_name = os.path.splitext(base_name)[0]\n",
    "            output_dir = os.path.dirname(original_file_path)\n",
    "            \n",
    "            # Save agent audio\n",
    "            agent_path = os.path.join(output_dir, f\"{file_name}_agent.wav\")\n",
    "            if len(speakers_dict['agent']) > 0:\n",
    "                sf.write(agent_path, speakers_dict['agent'], sample_rate)\n",
    "                print(f\"Agent audio saved to: {agent_path}\")\n",
    "            else:\n",
    "                print(\"No agent audio segments found\")\n",
    "                \n",
    "            # Save customer audio\n",
    "            customer_path = os.path.join(output_dir, f\"{file_name}_customer.wav\")\n",
    "            if len(speakers_dict['customer']) > 0:\n",
    "                sf.write(customer_path, speakers_dict['customer'], sample_rate)\n",
    "                print(f\"Customer audio saved to: {customer_path}\")\n",
    "            else:\n",
    "                print(\"No customer audio segments found\")\n",
    "                \n",
    "            return {\n",
    "                'agent_path': agent_path if len(speakers_dict['agent']) > 0 else None,\n",
    "                'customer_path': customer_path if len(speakers_dict['customer']) > 0 else None\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving speaker audio: {e}\")\n",
    "            return None\n",
    "\n",
    "    def extract_features(self, y, sample_rate):\n",
    "        if y is None or len(y) == 0:\n",
    "            return {k: 0 for k in ['pitch_mean', 'pitch_std', 'pitch_range', 'energy_mean', 'energy_std',\n",
    "                                   'energy_range', 'tempo', 'pause_ratio', 'speech_rate',\n",
    "                                   'spectral_centroid', 'spectral_rolloff']}\n",
    "\n",
    "        # Use larger hop_length for faster processing\n",
    "        hop_length = 512\n",
    "        features = {}\n",
    "        features['pitch'] = librosa.yin(y, fmin=50, fmax=500, hop_length=hop_length)\n",
    "        features['pitch_mean'] = np.mean(features['pitch'])\n",
    "        features['pitch_std'] = np.std(features['pitch'])\n",
    "        features['pitch_range'] = np.max(features['pitch']) - np.min(features['pitch'])\n",
    "\n",
    "        features['rms'] = librosa.feature.rms(y=y, hop_length=hop_length)[0]\n",
    "        features['energy_mean'] = np.mean(features['rms'])\n",
    "        features['energy_std'] = np.std(features['rms'])\n",
    "        features['energy_range'] = np.max(features['rms']) - np.min(features['rms'])\n",
    "\n",
    "        tempo, _ = librosa.beat.beat_track(y=y, sr=sample_rate, hop_length=hop_length)\n",
    "        features['tempo'] = tempo\n",
    "\n",
    "        non_silent_intervals = librosa.effects.split(y, top_db=20)\n",
    "        features['pause_ratio'] = 1 - (sum(i[1]-i[0] for i in non_silent_intervals) / len(y))\n",
    "        features['speech_rate'] = len(non_silent_intervals) / (len(y) / sample_rate)\n",
    "\n",
    "        features['spectral_centroid'] = np.mean(librosa.feature.spectral_centroid(y=y, sr=sample_rate, hop_length=hop_length)[0])\n",
    "        features['spectral_rolloff'] = np.mean(librosa.feature.spectral_rolloff(y=y, sr=sample_rate, hop_length=hop_length)[0])\n",
    "\n",
    "        return features\n",
    "\n",
    "    def get_llm_analysis(self, agent_features, customer_features):\n",
    "        llm_results = {}\n",
    "        feature_description = f\"\"\"\n",
    "        Agent audio features:\n",
    "        - Pitch (mean): {agent_features['pitch_mean']:.2f} Hz\n",
    "        - Pitch variation: {agent_features['pitch_std']:.2f} Hz\n",
    "        - Energy level: {agent_features['energy_mean']:.4f}\n",
    "        - Speech rate: {agent_features['speech_rate']:.2f} segments/sec\n",
    "        - Pause ratio: {agent_features['pause_ratio']:.2f}\n",
    "        - Voice quality (spectral centroid): {agent_features['spectral_centroid']:.2f}\n",
    "        Customer audio features:\n",
    "        - Pitch (mean): {customer_features['pitch_mean']:.2f} Hz\n",
    "        - Pitch variation: {customer_features['pitch_std']:.2f} Hz\n",
    "        - Energy level: {customer_features['energy_mean']:.4f}\n",
    "        - Speech rate: {customer_features['speech_rate']:.2f} segments/sec\n",
    "        - Pause ratio: {customer_features['pause_ratio']:.2f}\n",
    "        - Voice quality (spectral centroid): {customer_features['spectral_centroid']:.2f}\n",
    "\"\"\"\n",
    "        prompt = f\"{feature_description}\\n\\nBased on these audio features, please provide:\\n1. Customer sentiment score (0-10)\\n2. Agent politeness score (0-10)\\n3. Agent empathy score (0-10)\\nInclude a brief explanation for each score.\"\n",
    "        # Try OpenAI analysis \n",
    "        try:\n",
    "            if openai.api_key:\n",
    "                response = openai.chat.completions.create(\n",
    "                    model=\"gpt-4\",\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are an expert in analyzing voice features to determine emotional states and communication quality.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                        ]\n",
    "                        )\n",
    "                llm_results['openai'] = response.choices[0].message.content\n",
    "            else:\n",
    "                llm_results['openai'] = \"OpenAI API key not configured. Please set the OPENAI_API_KEY environment variable.\"\n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            llm_results['openai'] = f\"Error with OpenAI analysis: {error_msg}\"\n",
    "            print(f\"OpenAI API error: {error_msg}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "    # Try Gemini analysis\n",
    "        try:\n",
    "            if os.getenv(\"GOOGLE_API_KEY\"):\n",
    "                model = genai.GenerativeModel('gemini-pro')\n",
    "                response = model.generate_content(prompt)\n",
    "                llm_results['gemini'] = response.text\n",
    "            else:\n",
    "                llm_results['gemini'] = \"Google API key not configured. Please set the GOOGLE_API_KEY environment variable.\"\n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            llm_results['gemini'] = f\"Error with Gemini analysis: {error_msg}\"\n",
    "            print(f\"Gemini API error: {error_msg}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "        return llm_results\n",
    "\n",
    "    def load_scores_from_json(self, json_path):\n",
    "        \"\"\"Load scores that were calculated by the first cell\"\"\"\n",
    "        try:\n",
    "            with open(json_path, 'r') as f:\n",
    "                scores = json.load(f)\n",
    "            print(f\"Loaded scores from {json_path}\")\n",
    "            return scores\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading scores from {json_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def analyze_call(self, audio_path, scores_json_path=None):\n",
    "        y, sample_rate = self.process_audio(audio_path)\n",
    "        if y is None:\n",
    "            return {\n",
    "                'file_name': os.path.basename(audio_path),\n",
    "                'error': \"Failed to process audio file\"\n",
    "            }\n",
    "\n",
    "        speakers = self.diarize_speakers(y, sample_rate)\n",
    "        \n",
    "        # Save the separated audio files\n",
    "        audio_paths = self.save_speaker_audio(speakers, sample_rate, audio_path)\n",
    "\n",
    "        agent_features = self.extract_features(speakers['agent'], sample_rate)\n",
    "        customer_features = self.extract_features(speakers['customer'], sample_rate)\n",
    "\n",
    "        # First try to load scores from the first cell's JSON file\n",
    "        scores = None\n",
    "        if scores_json_path and os.path.exists(scores_json_path):\n",
    "            scores = self.load_scores_from_json(scores_json_path)\n",
    "        \n",
    "        if scores:\n",
    "            # Use the scores from the first cell if available\n",
    "            customer_sentiment = scores.get(\"Customer Sentiment Score\", 0)\n",
    "            agent_politeness = scores.get(\"Agent Politeness Score\", 0)\n",
    "            agent_empathy = scores.get(\"Agent Empathy Score\", 0)\n",
    "        else:\n",
    "            # As a fallback, calculate scores directly (original functionality)\n",
    "            agent_scores = self.calculate_scores(agent_features, 'agent')\n",
    "            customer_scores = self.calculate_scores(customer_features, 'customer')\n",
    "            customer_sentiment = customer_scores.get('sentiment', 0)\n",
    "            agent_politeness = agent_scores.get('politeness', 0)\n",
    "            agent_empathy = agent_scores.get('empathy', 0)\n",
    "\n",
    "        llm_analysis = self.get_llm_analysis(agent_features, customer_features)\n",
    "\n",
    "        results = {\n",
    "            'file_name': os.path.basename(audio_path),\n",
    "            'customer_sentiment': customer_sentiment,\n",
    "            'agent_politeness': agent_politeness,\n",
    "            'agent_empathy': agent_empathy,\n",
    "            'agent_features': agent_features,\n",
    "            'customer_features': customer_features,\n",
    "            'llm_analysis': llm_analysis,\n",
    "            'audio_paths': audio_paths  # Include the paths to the saved audio files\n",
    "        }\n",
    "\n",
    "        self._visualize_results(results)\n",
    "        return results\n",
    "\n",
    "    def calculate_scores(self, features, speaker_type):\n",
    "        \"\"\"Original score calculation as fallback\"\"\"\n",
    "        scores = {}\n",
    "\n",
    "        if speaker_type == 'customer':\n",
    "            scores['sentiment'] = np.clip(5 +\n",
    "                                         features['pitch_range'] / 50 +\n",
    "                                         features['energy_mean'] * 20 - \n",
    "                                         features['pause_ratio'] * 5, 0, 10)\n",
    "\n",
    "        if speaker_type == 'agent':\n",
    "            scores['politeness'] = np.clip(7 - \n",
    "                                          abs(features['tempo'] - 100) / 20 - \n",
    "                                          features['pitch_std'] / 10 + \n",
    "                                          features['pause_ratio'] * 5, 0, 10)\n",
    "\n",
    "            scores['empathy'] = np.clip(5 + \n",
    "                                       features['pitch_range'] / 40 + \n",
    "                                       features['energy_range'] * 10, 0, 10)\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def _visualize_results(self, results):\n",
    "        try:\n",
    "            plt.figure(figsize=(12, 10))\n",
    "\n",
    "            customer_sentiment = results.get('customer_sentiment') or 0\n",
    "            agent_politeness = results.get('agent_politeness') or 0\n",
    "            agent_empathy = results.get('agent_empathy') or 0\n",
    "\n",
    "            plt.subplot(221)\n",
    "            metrics = ['Customer Sentiment', 'Agent Politeness', 'Agent Empathy']\n",
    "            values = [float(customer_sentiment), float(agent_politeness), float(agent_empathy)]\n",
    "            plt.bar(metrics, values, color=['blue', 'green', 'purple'])\n",
    "            plt.ylim(0, 10)\n",
    "            plt.title('Call Analysis Scores')\n",
    "\n",
    "            plt.subplot(222)\n",
    "            features = ['pitch_mean', 'energy_mean', 'speech_rate']\n",
    "            agent_values = [float(results['agent_features'].get(f, 0)) for f in features]\n",
    "            customer_values = [float(results['customer_features'].get(f, 0)) for f in features]\n",
    "            x = np.arange(len(features))\n",
    "            width = 0.35\n",
    "\n",
    "            plt.bar(x - width / 2, agent_values, width, label='Agent')\n",
    "            plt.bar(x + width / 2, customer_values, width, label='Customer')\n",
    "            plt.xticks(x, [f.replace('_', ' ').title() for f in features])\n",
    "            plt.legend()\n",
    "            plt.title('Voice Feature Comparison')\n",
    "\n",
    "            plt.subplot(212)\n",
    "            plt.axis('off')\n",
    "            llm_text = \"LLM Analysis Summary:\\n\\n\"\n",
    "            for llm, analysis in results['llm_analysis'].items():\n",
    "                llm_text += f\"{llm.upper()}:\\n{(analysis or 'No response')[:300]}...\\n\\n\"\n",
    "            plt.text(0, 0.5, llm_text, fontsize=9, wrap=True)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(os.path.dirname(results['audio_paths'].get('agent_path', '')), \n",
    "                                    f\"{results['file_name']}_analysis.png\"))\n",
    "            plt.close()\n",
    "\n",
    "            print(f\"\\n=== Analysis for {results['file_name']} ===\")\n",
    "            print(f\"Customer Sentiment Score: {float(customer_sentiment):.2f}/10\")\n",
    "            print(f\"Agent Politeness Score: {float(agent_politeness):.2f}/10\")\n",
    "            print(f\"Agent Empathy Score: {float(agent_empathy):.2f}/10\")\n",
    "            \n",
    "            # Print the paths to the saved audio files\n",
    "            if results.get('audio_paths'):\n",
    "                print(f\"\\nSeparated audio files:\")\n",
    "                if results['audio_paths'].get('agent_path'):\n",
    "                    print(f\"Agent audio: {results['audio_paths']['agent_path']}\")\n",
    "                if results['audio_paths'].get('customer_path'):\n",
    "                    print(f\"Customer audio: {results['audio_paths']['customer_path']}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[Visualization Error] Skipping chart for {results.get('file_name')}: {str(e)}\")\n",
    "\n",
    "# Helper function to save features to JSON\n",
    "def save_features_to_json(features, output_path='extracted_features.json'):\n",
    "    serializable_features = {}\n",
    "    for key, df in features.items():\n",
    "        serializable_features[key] = df.to_dict(orient='records')[0]\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(serializable_features, f, indent=4)\n",
    "    print(f\"Features saved to {output_path}\")\n",
    "\n",
    "# Test the enhanced functionality\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    audio_processor = AudioProcessor()\n",
    "    call_analyzer = CallAnalyzer()\n",
    "    \n",
    "    # Test with a sample file\n",
    "    test_file = \"/Users/vikaskhare/Documents/sample_call.wav\"\n",
    "    if os.path.exists(test_file):\n",
    "        # Process audio with noise reduction\n",
    "        processed_file, y, sample_rate = audio_processor.process_audio(test_file)\n",
    "        \n",
    "        # Perform enhanced speaker diarization\n",
    "        if processed_file:\n",
    "            result = call_analyzer.analyze_call(processed_file)\n",
    "            print(\"Analysis complete!\")\n",
    "    else:\n",
    "        print(f\"Test file {test_file} not found. Please provide a valid audio file path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44a0110a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in /opt/homebrew/lib/python3.11/site-packages (6.5.1)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in /opt/homebrew/lib/python3.11/site-packages (from gradio) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /opt/homebrew/lib/python3.11/site-packages (from gradio) (4.12.1)\n",
      "Requirement already satisfied: brotli>=1.1.0 in /opt/homebrew/lib/python3.11/site-packages (from gradio) (1.2.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /opt/homebrew/lib/python3.11/site-packages (from gradio) (0.128.1)\n",
      "Requirement already satisfied: ffmpy in /opt/homebrew/lib/python3.11/site-packages (from gradio) (1.0.0)\n",
      "Requirement already satisfied: gradio-client==2.0.3 in /opt/homebrew/lib/python3.11/site-packages (from gradio) (2.0.3)\n",
      "Requirement already satisfied: groovy~=0.1 in /opt/homebrew/lib/python3.11/site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx<1.0,>=0.24.1 in /opt/homebrew/lib/python3.11/site-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /opt/homebrew/lib/python3.11/site-packages (from gradio) (1.4.0)\n",
      "Requirement already satisfied: jinja2<4.0 in /opt/homebrew/lib/python3.11/site-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from gradio) (3.0.3)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in /opt/homebrew/lib/python3.11/site-packages (from gradio) (2.3.5)\n",
      "Requirement already satisfied: orjson~=3.0 in /opt/homebrew/lib/python3.11/site-packages (from gradio) (3.11.7)\n",
      "Requirement already satisfied: packaging in /Users/vikaskhare/Library/Python/3.11/lib/python/site-packages (from gradio) (26.0)\n",
      "Requirement already satisfied: pandas<4.0,>=1.0 in /opt/homebrew/lib/python3.11/site-packages (from gradio) (3.0.0)\n",
      "Requirement already satisfied: pillow<13.0,>=8.0 in /opt/homebrew/lib/python3.11/site-packages (from gradio) (12.1.0)\n",
      "Requirement already satisfied: pydantic<=3.0,>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from gradio) (2.12.5)\n",
      "Requirement already satisfied: pydub in /opt/homebrew/lib/python3.11/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /opt/homebrew/lib/python3.11/site-packages (from gradio) (0.0.22)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/homebrew/lib/python3.11/site-packages (from gradio) (2025.2)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /opt/homebrew/lib/python3.11/site-packages (from gradio) (6.0.3)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.7 in /opt/homebrew/lib/python3.11/site-packages (from gradio) (0.1.7)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /opt/homebrew/lib/python3.11/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /opt/homebrew/lib/python3.11/site-packages (from gradio) (0.50.0)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /opt/homebrew/lib/python3.11/site-packages (from gradio) (0.13.3)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /opt/homebrew/lib/python3.11/site-packages (from gradio) (0.21.1)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /Users/vikaskhare/Library/Python/3.11/lib/python/site-packages (from gradio) (4.15.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /opt/homebrew/lib/python3.11/site-packages (from gradio) (0.40.0)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/lib/python3.11/site-packages (from gradio-client==2.0.3->gradio) (2026.1.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/homebrew/lib/python3.11/site-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /opt/homebrew/lib/python3.11/site-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.4)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/lib/python3.11/site-packages (from httpx<1.0,>=0.24.1->gradio) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/lib/python3.11/site-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/homebrew/lib/python3.11/site-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
      "Requirement already satisfied: shellingham in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.5.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.2)\n",
      "Requirement already satisfied: typer-slim in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (0.21.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/vikaskhare/Library/Python/3.11/lib/python/site-packages (from pandas<4.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/homebrew/lib/python3.11/site-packages (from pydantic<=3.0,>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/homebrew/lib/python3.11/site-packages (from pydantic<=3.0,>=2.0->gradio) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/homebrew/lib/python3.11/site-packages (from pydantic<=3.0,>=2.0->gradio) (0.4.2)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/homebrew/lib/python3.11/site-packages (from typer<1.0,>=0.12->gradio) (8.3.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/homebrew/lib/python3.11/site-packages (from typer<1.0,>=0.12->gradio) (14.3.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/vikaskhare/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas<4.0,>=1.0->gradio) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/homebrew/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/vikaskhare/Library/Python/3.11/lib/python/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/homebrew/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.11/bin/python3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: resampy in /opt/homebrew/lib/python3.11/site-packages (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/lib/python3.11/site-packages (from resampy) (2.3.5)\n",
      "Requirement already satisfied: numba>=0.53 in /opt/homebrew/lib/python3.11/site-packages (from resampy) (0.63.1)\n",
      "Requirement already satisfied: llvmlite<0.47,>=0.46.0dev0 in /opt/homebrew/lib/python3.11/site-packages (from numba>=0.53->resampy) (0.46.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.11/bin/python3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gradio\n",
    "import sys\n",
    "!{sys.executable} -m pip install resampy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d599c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/gradio/interface.py:171: UserWarning: The parameters have been moved from the Blocks constructor to the launch() method in Gradio 6.0: theme. Please pass these parameters to launch() instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7867\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7867/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing audio file: /private/var/folders/g_/zzf2f_250pb_mndpb1ygp24w0000gn/T/gradio/87c2ba7481a671eb4878351c837819221dae7fd471ff8b03d39541c9ca66f1d8/4504.wav\n",
      "Loading /private/var/folders/g_/zzf2f_250pb_mndpb1ygp24w0000gn/T/gradio/87c2ba7481a671eb4878351c837819221dae7fd471ff8b03d39541c9ca66f1d8/4504.wav...\n",
      "Long file detected (470.76 seconds), optimizing processing...\n",
      "Original: Sample Rate = 22050 Hz, Duration = 470.76 seconds\n",
      "Applying noise reduction...\n",
      "Noise reduction complete\n",
      "Noise reduced audio saved to /private/var/folders/g_/zzf2f_250pb_mndpb1ygp24w0000gn/T/gradio/87c2ba7481a671eb4878351c837819221dae7fd471ff8b03d39541c9ca66f1d8/4504_noise_reduced.wav\n",
      "Resampling to 16000 Hz...\n",
      "Processed audio saved to /private/var/folders/g_/zzf2f_250pb_mndpb1ygp24w0000gn/T/gradio/87c2ba7481a671eb4878351c837819221dae7fd471ff8b03d39541c9ca66f1d8/4504_16khz.wav\n",
      "Extracting custom OpenSMILE-style features...\n",
      "Extracting Praat-like features...\n",
      "Features saved to /private/var/folders/g_/zzf2f_250pb_mndpb1ygp24w0000gn/T/gradio/87c2ba7481a671eb4878351c837819221dae7fd471ff8b03d39541c9ca66f1d8/4504_16khz_features.json\n",
      "Diarizing speakers for 4504_16khz.wav...\n",
      "Performing enhanced speaker diarization...\n",
      "Speech recognition error: \n",
      "Speech recognition error: \n",
      "Speech recognition error: \n",
      "Speech recognition error: \n",
      "Speech recognition error: \n",
      "Speech recognition error: \n",
      "Speech recognition error: \n",
      "Speech recognition error: \n",
      "Speech recognition error: \n",
      "Speech recognition error: \n",
      "Speech recognition error: \n",
      "Speech recognition error: \n",
      "Speech recognition error: \n",
      "Speech recognition error: \n",
      "Speech recognition error: \n",
      "Speech recognition error: \n",
      "Speech recognition error: \n",
      "Speech recognition error: \n",
      "Speech recognition error: \n",
      "Speech recognition error: \n",
      "Speech recognition error: \n",
      "Speech recognition error: \n",
      "Speech recognition error: \n",
      "Speech recognition error: \n",
      "Speech recognition error: \n",
      "Speech recognition error: \n",
      "Speech recognition error: \n",
      "Speech recognition error: \n",
      "Speech recognition error: \n",
      "Speech recognition error: \n",
      "Speech recognition error: \n",
      "Speech recognition error: \n",
      "Speech recognition error: \n",
      "Speech recognition error: \n",
      "Speech recognition error: \n",
      "Speech recognition error: \n",
      "Speech recognition error: \n",
      "Speech recognition error: \n",
      "Speech recognition error: \n",
      "Speech recognition error: \n",
      "Speech recognition error: \n",
      "Speaker identification scores - Speaker 0: 1, Speaker 1: 3\n",
      "Identified agent as Speaker 1, customer as Speaker 0\n",
      "Diarization complete. Agent segments: 2806272, Customer segments: 4317824\n",
      "Agent audio saved to: /private/var/folders/g_/zzf2f_250pb_mndpb1ygp24w0000gn/T/gradio/87c2ba7481a671eb4878351c837819221dae7fd471ff8b03d39541c9ca66f1d8/4504_16khz_agent.wav\n",
      "Customer audio saved to: /private/var/folders/g_/zzf2f_250pb_mndpb1ygp24w0000gn/T/gradio/87c2ba7481a671eb4878351c837819221dae7fd471ff8b03d39541c9ca66f1d8/4504_16khz_customer.wav\n",
      "Extracting features for each speaker...\n",
      "Loaded scores from /private/var/folders/g_/zzf2f_250pb_mndpb1ygp24w0000gn/T/gradio/87c2ba7481a671eb4878351c837819221dae7fd471ff8b03d39541c9ca66f1d8/4504_16khz_scores.json\n",
      "Getting LLM analysis...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import gradio as gr\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import json\n",
    "import soundfile as sf\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import google.generativeai as genai\n",
    "import traceback\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.signal import medfilt\n",
    "import io\n",
    "import base64\n",
    "import noisereduce as nr\n",
    "import speech_recognition as speech_rec  # Renamed to avoid conflict\n",
    "import langid\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure API keys\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# Constants\n",
    "TARGET_SAMPLE_RATE = 16000\n",
    "OUTPUT_DIR = 'processed_audio'\n",
    "\n",
    "# region AudioProcessor\n",
    "class AudioProcessor:\n",
    "    def __init__(self, target_sr=TARGET_SAMPLE_RATE, output_dir=OUTPUT_DIR):\n",
    "        self.target_sr = target_sr\n",
    "        self.output_dir = output_dir  # Will be updated based on input file\n",
    "        \n",
    "    def reduce_noise(self, y, sample_rate):\n",
    "        \"\"\"Apply noise reduction to clean the audio\"\"\"\n",
    "        print(\"Applying noise reduction...\")\n",
    "        # Apply noise reduction with optimized parameters\n",
    "        reduced_noise = nr.reduce_noise(\n",
    "            y=y, \n",
    "            sr=sample_rate,\n",
    "            stationary=True,\n",
    "            prop_decrease=0.75\n",
    "        )\n",
    "        print(\"Noise reduction complete\")\n",
    "        return reduced_noise\n",
    "        \n",
    "    def process_audio(self, file_path):\n",
    "        file_path = file_path.strip('\"')\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Error: File {file_path} not found.\")\n",
    "            return None, None, None, None\n",
    "        \n",
    "        # Set output directory to same folder as input\n",
    "        self.output_dir = os.path.dirname(file_path)\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        \n",
    "        filename = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        output_path = os.path.join(self.output_dir, f\"{filename}_16khz.wav\")\n",
    "        noise_reduced_path = os.path.join(self.output_dir, f\"{filename}_noise_reduced.wav\")\n",
    "\n",
    "        print(f\"Loading {file_path}...\")\n",
    "        try:\n",
    "            # Load with a lower sample rate for faster processing if file is large\n",
    "            y, sample_rate = librosa.load(file_path, sr=None)\n",
    "            file_duration = librosa.get_duration(y=y, sr=sample_rate)\n",
    "            \n",
    "            # For very long files, consider downsampling during load\n",
    "            if file_duration > 300:  # If longer than 5 minutes\n",
    "                print(f\"Long file detected ({file_duration:.2f} seconds), optimizing processing...\")\n",
    "                y, sample_rate = librosa.load(file_path, sr=22050)  # Lower sample rate for initial processing\n",
    "        except Exception as e:\n",
    "            print(f\"Librosa failed to load {file_path}: {e}\")\n",
    "            return None, None, None, None\n",
    "\n",
    "        print(f\"Original: Sample Rate = {sample_rate} Hz, Duration = {librosa.get_duration(y=y, sr=sample_rate):.2f} seconds\")\n",
    "        \n",
    "        # Apply noise reduction\n",
    "        y_reduced = self.reduce_noise(y, sample_rate)\n",
    "        \n",
    "        # Save noise reduced audio\n",
    "        sf.write(noise_reduced_path, y_reduced, sample_rate, subtype='PCM_16')\n",
    "        print(f\"Noise reduced audio saved to {noise_reduced_path}\")\n",
    "        \n",
    "        if sample_rate != self.target_sr:\n",
    "            print(f\"Resampling to {self.target_sr} Hz...\")\n",
    "            # Use faster resampling method\n",
    "            y_reduced = librosa.resample(y_reduced, orig_sr=sample_rate, target_sr=self.target_sr, res_type='fft')\n",
    "\n",
    "        sf.write(output_path, y_reduced, self.target_sr, subtype='PCM_16')\n",
    "        print(f\"Processed audio saved to {output_path}\")\n",
    "        return output_path, y_reduced, self.target_sr, noise_reduced_path\n",
    "# endregion\n",
    "\n",
    "# region FeatureExtractor\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, sample_rate=TARGET_SAMPLE_RATE):\n",
    "        self.sample_rate = sample_rate\n",
    "\n",
    "    def extract_all_features(self, audio_path, y, sample_rate):\n",
    "        # Use a more efficient approach for feature extraction\n",
    "        opensmile_features = self.extract_custom_opensmile(y, sample_rate)\n",
    "        praat_features = self.extract_praat_features(y, sample_rate)\n",
    "        \n",
    "        return {\n",
    "            'opensmile': opensmile_features,\n",
    "            'praat': praat_features\n",
    "        }\n",
    "\n",
    "    def extract_custom_opensmile(self, y, sample_rate):\n",
    "        print(\"Extracting custom OpenSMILE-style features...\")\n",
    "        # Optimize pitch extraction for performance\n",
    "        hop_length = 512  # Larger hop length for faster processing\n",
    "        pitch, voiced_flag, _ = librosa.pyin(\n",
    "            y, \n",
    "            fmin=75, \n",
    "            fmax=400, \n",
    "            sr=sample_rate, \n",
    "            hop_length=hop_length,\n",
    "            fill_na=0.0\n",
    "        )\n",
    "        \n",
    "        pitch_valid = pitch[voiced_flag]\n",
    "        jitter = 0.01\n",
    "        if len(pitch_valid) > 1:\n",
    "            pitch_diffs = np.abs(np.diff(pitch_valid))\n",
    "            jitter = np.mean(pitch_diffs) / np.mean(pitch_valid) if np.mean(pitch_valid) > 0 else 0.01\n",
    "\n",
    "        rms = librosa.feature.rms(y=y, hop_length=hop_length)[0]\n",
    "        shimmer = 0.04\n",
    "        if len(rms) > 1:\n",
    "            rms_diffs = np.abs(np.diff(rms))\n",
    "            shimmer = np.mean(rms_diffs) / np.mean(rms) if np.mean(rms) > 0 else 0.04\n",
    "\n",
    "        features = {\n",
    "            'loudness': np.mean(rms),\n",
    "            'energy': np.sum(y**2) / len(y),\n",
    "            'pitch_mean': np.mean(pitch_valid) if np.any(voiced_flag) else 0,\n",
    "            'pitch_range': np.ptp(pitch_valid) if np.any(voiced_flag) else 0,\n",
    "            'jitter': jitter,\n",
    "            'shimmer': shimmer\n",
    "        }\n",
    "        return pd.DataFrame([features])\n",
    "\n",
    "    def extract_praat_features(self, y, sample_rate):\n",
    "        print(\"Extracting Praat-like features...\")\n",
    "        # Use larger hop length for faster processing\n",
    "        hop_length = 512\n",
    "        \n",
    "        pitch, voiced_flag, _ = librosa.pyin(\n",
    "            y, \n",
    "            fmin=75, \n",
    "            fmax=400, \n",
    "            sr=sample_rate, \n",
    "            hop_length=hop_length,\n",
    "            fill_na=0.0\n",
    "        )\n",
    "        \n",
    "        # Optimize STFT calculation\n",
    "        n_fft = 2048  # Larger FFT size for better frequency resolution\n",
    "        S = np.abs(librosa.stft(y, n_fft=n_fft, hop_length=hop_length))\n",
    "        spectral_flatness = librosa.feature.spectral_flatness(S=S)[0]\n",
    "        hnr_estimate = 20 * (1 - np.mean(spectral_flatness))\n",
    "        intensity = np.mean(librosa.feature.rms(y=y, hop_length=hop_length)[0]) * 100\n",
    "\n",
    "        onset_env = librosa.onset.onset_strength(y=y, sr=sample_rate, hop_length=hop_length)\n",
    "        peaks = librosa.util.peak_pick(onset_env, pre_max=3, post_max=3, pre_avg=3, post_avg=3, delta=0.5, wait=10)\n",
    "\n",
    "        duration = len(y) / sample_rate\n",
    "        speaking_rate = len(peaks) / duration if duration > 0 else 4.0\n",
    "        speaking_rate = max(3.0, min(5.5, speaking_rate))\n",
    "\n",
    "        features = {\n",
    "            'F0_mean': np.mean(pitch[voiced_flag]) if np.any(voiced_flag) else 0,\n",
    "            'F0_std': np.std(pitch[voiced_flag]) if np.any(voiced_flag) else 0,\n",
    "            'HNR': hnr_estimate,\n",
    "            'Intensity_mean': intensity,\n",
    "            'Speaking_rate': speaking_rate\n",
    "        }\n",
    "        return pd.DataFrame([features])\n",
    "# endregion\n",
    "\n",
    "# region VoiceAnalyzer\n",
    "class VoiceAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.sentiment_mapping = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "        self.politeness_mapping = {0: \"Impolite\", 1: \"Neutral\", 2: \"Polite\"}\n",
    "        self.empathy_mapping = {0: \"Low Empathy\", 1: \"Moderate Empathy\", 2: \"High Empathy\"}\n",
    "\n",
    "    def extract_indicators(self, features):\n",
    "        indicators = {}\n",
    "        if 'opensmile' in features:\n",
    "            osm = features['opensmile']\n",
    "            indicators.update({k: osm[k].values[0] for k in osm.columns})\n",
    "        if 'praat' in features:\n",
    "            praat = features['praat']\n",
    "            indicators.update({k.lower(): praat[k].values[0] for k in praat.columns})\n",
    "        return indicators\n",
    "\n",
    "    def analyze_voice(self, indicators):\n",
    "        results = {}\n",
    "        s_base, p_base, e_base = 5.0, 5.0, 5.0\n",
    "        e = indicators['energy']\n",
    "        pr = indicators['pitch_range']\n",
    "        j = indicators['jitter']\n",
    "        sr = indicators['speaking_rate']\n",
    "        i = indicators['intensity_mean']\n",
    "        f0 = indicators['f0_mean']\n",
    "        h = indicators['hnr']\n",
    "        sh = indicators['shimmer']\n",
    "\n",
    "        sentiment_score = s_base + min(2.0, e*10) + min(2.0, pr/50) + max(0, 1.0 - j*50) + (\n",
    "            2.0 if 4.0 <= sr <= 5.0 else 1.5 if 3.5 <= sr < 4.0 or 5.0 < sr <= 5.5 else 1.0)\n",
    "        sentiment_score = max(1.0, min(10.0, sentiment_score))\n",
    "        results['sentiment'] = {\n",
    "            'score': round(sentiment_score, 1),\n",
    "            'description': self._get_score_description(sentiment_score, \"sentiment\")\n",
    "        }\n",
    "\n",
    "        p_score = p_base + (\n",
    "            2.0 if pr < 50 else 1.5 if pr < 80 else 1.0 if pr < 120 else 0) + (\n",
    "            2.0 if i < 60 else 1.5 if i < 70 else 1.0 if i < 80 else 0) + (\n",
    "            2.0 if 3.8 <= sr <= 4.5 else 1.5 if 3.5 <= sr < 3.8 or 4.5 < sr <= 5.0 else 0.5) + min(1.0, h / 20)\n",
    "        p_score = max(1.0, min(10.0, p_score))\n",
    "        results['politeness'] = {\n",
    "            'score': round(p_score, 1),\n",
    "            'description': self._get_score_description(p_score, \"politeness\")\n",
    "        }\n",
    "\n",
    "        e_score = e_base + (\n",
    "            1.5 if 180 < f0 < 280 else 0) + (\n",
    "            1.0 if 30 < pr < 90 and 180 < f0 < 280 else 0) + (\n",
    "            1.5 if h > 15 else 1.0 if h > 10 else 0) + (\n",
    "            1.5 if 3.8 <= sr <= 4.5 else 1.0 if 3.5 <= sr < 3.8 or 4.5 < sr <= 5.0 else 0) + (\n",
    "            1.0 if 0.03 <= sh <= 0.06 else 0)\n",
    "        e_score = max(1.0, min(10.0, e_score))\n",
    "        results['empathy'] = {\n",
    "            'score': round(e_score, 1),\n",
    "            'description': self._get_score_description(e_score, \"empathy\")\n",
    "        }\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _get_score_description(self, score, attribute):\n",
    "        bins = {\n",
    "            \"sentiment\": [\"Very negative\", \"Negative\", \"Slightly negative\", \"Neutral\", \"Slightly positive\", \"Positive\", \"Very positive\"],\n",
    "            \"politeness\": [\"Impolite\", \"Direct/abrupt\", \"Somewhat direct\", \"Neutral\", \"Polite\", \"Very polite\", \"Extremely polite\"],\n",
    "            \"empathy\": [\"Very distant/cold\", \"Distant\", \"Somewhat distant\", \"Moderately empathetic\", \"Empathetic\", \"Very empathetic\", \"Highly empathetic\"]\n",
    "        }\n",
    "        idx = int(min(6, max(0, round(score // 1.5))))\n",
    "        return bins.get(attribute, [\"Unknown\"])[idx]\n",
    "\n",
    "    def analyze(self, features):\n",
    "        indicators = self.extract_indicators(features)\n",
    "        results = self.analyze_voice(indicators)\n",
    "        return results, indicators\n",
    "# endregion\n",
    "\n",
    "# region CallAnalyzer \n",
    "class CallAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.recognizer = speech_rec.Recognizer()  # Fixed: Use renamed module\n",
    "        \n",
    "    def process_audio(self, audio_path):\n",
    "        try:\n",
    "            y, sample_rate = librosa.load(audio_path, sr=None)  # Renamed to avoid conflict\n",
    "            return y, sample_rate\n",
    "        except Exception as e:\n",
    "            print(f\"Audio processing error: {e}\")\n",
    "            return None, None\n",
    "            \n",
    "    def detect_language(self, audio_segment, sample_rate):\n",
    "        \"\"\"Detect language of an audio segment\"\"\"\n",
    "        try:\n",
    "            # Convert numpy array to bytes for speech recognition\n",
    "            audio_bytes = (audio_segment * 32767).astype(np.int16).tobytes()\n",
    "        \n",
    "            # Create AudioData with the correct parameters\n",
    "            sample_width = 2  # 16-bit audio = 2 bytes per sample\n",
    "            audio_data = speech_rec.AudioData(audio_bytes, sample_rate, sample_width)  # Fixed: Use renamed module\n",
    "        \n",
    "            # Try to recognize text\n",
    "            try:\n",
    "                text = self.recognizer.recognize_google(audio_data)\n",
    "                # Detect language from text\n",
    "                lang, _ = langid.classify(text)\n",
    "                return lang\n",
    "            except Exception as e:\n",
    "                print(f\"Speech recognition error: {e}\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"Language detection error: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "    def extract_segment_features(self, segment, sample_rate):\n",
    "        \"\"\"Extract features from a segment to help with speaker identification\"\"\"\n",
    "        if len(segment) < sample_rate * 0.5:  # Skip segments shorter than 0.5 seconds\n",
    "            return None\n",
    "            \n",
    "        features = {}\n",
    "        \n",
    "        # Extract pitch with larger hop_length for faster processing\n",
    "        try:\n",
    "            hop_length = 512\n",
    "            pitch, voiced_flag, _ = librosa.pyin(\n",
    "                segment, \n",
    "                fmin=75, \n",
    "                fmax=400, \n",
    "                sr=sample_rate, \n",
    "                hop_length=hop_length,\n",
    "                fill_na=0.0\n",
    "            )\n",
    "            features['pitch_mean'] = np.mean(pitch[voiced_flag]) if np.any(voiced_flag) else 0\n",
    "        except:\n",
    "            features['pitch_mean'] = 0\n",
    "            \n",
    "        # Extract energy\n",
    "        try:\n",
    "            rms = librosa.feature.rms(y=segment, hop_length=hop_length)[0]\n",
    "            features['energy_mean'] = np.mean(rms)\n",
    "        except:\n",
    "            features['energy_mean'] = 0\n",
    "            \n",
    "        # Extract spectral features\n",
    "        try:\n",
    "            mfcc = librosa.feature.mfcc(y=segment, sr=sample_rate, n_mfcc=13, hop_length=hop_length)\n",
    "            features['mfcc_mean'] = np.mean(mfcc, axis=1)\n",
    "        except:\n",
    "            features['mfcc_mean'] = np.zeros(13)\n",
    "            \n",
    "        return features\n",
    "\n",
    "    def diarize_speakers(self, y, sample_rate):\n",
    "        \"\"\"\n",
    "        Improved speaker diarization with better segmentation and speaker identification\n",
    "        \"\"\"\n",
    "        print(\"Performing enhanced speaker diarization...\")\n",
    "        \n",
    "        # Step 1: Extract more robust features for segmentation\n",
    "        # Combine MFCCs with spectral contrast for better speaker differentiation\n",
    "        hop_length = 512\n",
    "        n_mfcc = 13\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sample_rate, n_mfcc=n_mfcc, hop_length=hop_length).T\n",
    "        \n",
    "        # Add spectral contrast features for better voice characteristic capture\n",
    "        contrast = librosa.feature.spectral_contrast(y=y, sr=sample_rate, hop_length=hop_length).T\n",
    "        \n",
    "        # Combine features\n",
    "        features = np.hstack([mfcc, contrast[:, :3]])  # Use first 3 contrast bands\n",
    "        \n",
    "        # Step 2: Use K-means for initial clustering with better initialization\n",
    "        # Try multiple initializations to find the best clustering\n",
    "        best_inertia = float('inf')\n",
    "        best_labels = None\n",
    "        \n",
    "        for i in range(3):  # Try 3 different initializations\n",
    "            kmeans = KMeans(n_clusters=2, random_state=i, n_init=2, max_iter=100).fit(features)\n",
    "            if kmeans.inertia_ < best_inertia:\n",
    "                best_inertia = kmeans.inertia_\n",
    "                best_labels = kmeans.labels_\n",
    "        \n",
    "        # Apply median filter to smooth labels and remove noise\n",
    "        labels = medfilt(best_labels, kernel_size=15)  # Increased kernel size for better smoothing\n",
    "        \n",
    "        # Step 3: Find segments with minimum duration\n",
    "        segments = []\n",
    "        current_label = labels[0]\n",
    "        start = 0\n",
    "        min_segment_frames = int(0.5 * sample_rate / hop_length)  # Minimum 0.5 seconds\n",
    "        \n",
    "        for i, label in enumerate(labels):\n",
    "            if label != current_label:\n",
    "                # Only keep segments longer than minimum duration\n",
    "                if i - start >= min_segment_frames:\n",
    "                    segments.append({\n",
    "                        'start': start,\n",
    "                        'end': i,\n",
    "                        'label': current_label,\n",
    "                        'start_time': start * hop_length / sample_rate,\n",
    "                        'end_time': i * hop_length / sample_rate\n",
    "                    })\n",
    "                start = i\n",
    "                current_label = label\n",
    "                \n",
    "        # Add the last segment\n",
    "        if len(labels) - start >= min_segment_frames:\n",
    "            segments.append({\n",
    "                'start': start,\n",
    "                'end': len(labels),\n",
    "                'label': current_label,\n",
    "                'start_time': start * hop_length / sample_rate,\n",
    "                'end_time': len(labels) * hop_length / sample_rate\n",
    "            })\n",
    "        \n",
    "        # Step 4: Extract audio for each segment and compute features\n",
    "        for segment in segments:\n",
    "            segment_start_sample = int(segment['start_time'] * sample_rate)\n",
    "            segment_end_sample = int(segment['end_time'] * sample_rate)\n",
    "            segment['audio'] = y[segment_start_sample:segment_end_sample]\n",
    "            \n",
    "            # Extract additional features for each segment\n",
    "            segment['features'] = self.extract_segment_features(segment['audio'], sample_rate)\n",
    "            \n",
    "            # Only detect language for longer segments\n",
    "            if len(segment['audio']) > sample_rate * 2:  # Only for segments > 2 seconds\n",
    "                segment['language'] = self.detect_language(segment['audio'], sample_rate)\n",
    "            else:\n",
    "                segment['language'] = None\n",
    "        \n",
    "        # Step 5: Improved speaker identification\n",
    "        # Analyze all segments to determine speaker characteristics\n",
    "        speaker_0_segments = [s for s in segments if s['label'] == 0 and s['features'] is not None]\n",
    "        speaker_1_segments = [s for s in segments if s['label'] == 1 and s['features'] is not None]\n",
    "        \n",
    "        # Skip analysis if we don't have enough segments for either speaker\n",
    "        if len(speaker_0_segments) < 2 or len(speaker_1_segments) < 2:\n",
    "            print(\"Not enough segments for reliable speaker identification. Using default assignment.\")\n",
    "            agent_label = 0  # Default: first speaker is agent\n",
    "            customer_label = 1\n",
    "        else:\n",
    "            # Extract key features for each speaker\n",
    "            speaker_0_pitch = np.mean([s['features']['pitch_mean'] for s in speaker_0_segments])\n",
    "            speaker_1_pitch = np.mean([s['features']['pitch_mean'] for s in speaker_1_segments])\n",
    "            \n",
    "            speaker_0_energy = np.mean([s['features']['energy_mean'] for s in speaker_0_segments])\n",
    "            speaker_1_energy = np.mean([s['features']['energy_mean'] for s in speaker_1_segments])\n",
    "            \n",
    "            # Calculate speaking time for each speaker\n",
    "            speaker_0_duration = sum([s['end_time'] - s['start_time'] for s in speaker_0_segments])\n",
    "            speaker_1_duration = sum([s['end_time'] - s['start_time'] for s in speaker_1_segments])\n",
    "            \n",
    "            # Check for language detection results\n",
    "            speaker_0_languages = [s['language'] for s in speaker_0_segments if s.get('language')]\n",
    "            speaker_1_languages = [s['language'] for s in speaker_1_segments if s.get('language')]\n",
    "            \n",
    "            # Determine which speaker is likely the agent based on multiple factors\n",
    "            agent_score_0 = 0\n",
    "            agent_score_1 = 0\n",
    "            \n",
    "            # 1. Agents typically have more consistent pitch (lower is better)\n",
    "            pitch_std_0 = np.std([s['features']['pitch_mean'] for s in speaker_0_segments])\n",
    "            pitch_std_1 = np.std([s['features']['pitch_mean'] for s in speaker_1_segments])\n",
    "            \n",
    "            if pitch_std_0 < pitch_std_1:\n",
    "                agent_score_0 += 1\n",
    "            else:\n",
    "                agent_score_1 += 1\n",
    "                \n",
    "            # 2. Agents typically speak more in customer service calls\n",
    "            if speaker_0_duration > speaker_1_duration:\n",
    "                agent_score_0 += 1\n",
    "            else:\n",
    "                agent_score_1 += 1\n",
    "                \n",
    "            # 3. Agents typically have moderate energy levels (not too high, not too low)\n",
    "            # Normalize energy to 0-1 range for comparison\n",
    "            max_energy = max(speaker_0_energy, speaker_1_energy)\n",
    "            if max_energy > 0:\n",
    "                norm_energy_0 = speaker_0_energy / max_energy\n",
    "                norm_energy_1 = speaker_1_energy / max_energy\n",
    "                \n",
    "                # Ideal agent energy is around 0.6-0.8 of max\n",
    "                if abs(norm_energy_0 - 0.7) < abs(norm_energy_1 - 0.7):\n",
    "                    agent_score_0 += 1\n",
    "                else:\n",
    "                    agent_score_1 += 1\n",
    "            \n",
    "            # 4. Check if non-English language is detected (customer more likely to speak non-English)\n",
    "            if speaker_0_languages and speaker_1_languages:\n",
    "                non_english_0 = sum(1 for lang in speaker_0_languages if lang != 'en')\n",
    "                non_english_1 = sum(1 for lang in speaker_1_languages if lang != 'en')\n",
    "                \n",
    "                if non_english_0 > non_english_1:\n",
    "                    # More non-English in speaker 0, so speaker 1 is more likely the agent\n",
    "                    agent_score_1 += 2  # Give this a higher weight\n",
    "                elif non_english_1 > non_english_0:\n",
    "                    # More non-English in speaker 1, so speaker 0 is more likely the agent\n",
    "                    agent_score_0 += 2  # Give this a higher weight\n",
    "            \n",
    "            # 5. Check who speaks first (agents typically speak first in calls)\n",
    "            if segments and len(segments) > 0:\n",
    "                first_speaker = segments[0]['label']\n",
    "                if first_speaker == 0:\n",
    "                    agent_score_0 += 1\n",
    "                else:\n",
    "                    agent_score_1 += 1\n",
    "            \n",
    "            # Determine agent based on scores\n",
    "            if agent_score_0 >= agent_score_1:\n",
    "                agent_label = 0\n",
    "                customer_label = 1\n",
    "            else:\n",
    "                agent_label = 1\n",
    "                customer_label = 0\n",
    "                \n",
    "            print(f\"Speaker identification scores - Speaker 0: {agent_score_0}, Speaker 1: {agent_score_1}\")\n",
    "            print(f\"Identified agent as Speaker {agent_label}, customer as Speaker {customer_label}\")\n",
    "        \n",
    "        # Step 6: Group segments by speaker with improved accuracy\n",
    "        agent_audio = []\n",
    "        customer_audio = []\n",
    "        \n",
    "        for segment in segments:\n",
    "            if segment['label'] == agent_label:\n",
    "                agent_audio.append(segment['audio'])\n",
    "            else:\n",
    "                customer_audio.append(segment['audio'])\n",
    "        \n",
    "        # Combine segments for each speaker\n",
    "        agent_audio = np.concatenate(agent_audio) if agent_audio else np.array([])\n",
    "        customer_audio = np.concatenate(customer_audio) if customer_audio else np.array([])\n",
    "        \n",
    "        print(f\"Diarization complete. Agent segments: {len(agent_audio)}, Customer segments: {len(customer_audio)}\")\n",
    "        return {\n",
    "            'agent': agent_audio,\n",
    "            'customer': customer_audio\n",
    "        }\n",
    "        \n",
    "    def save_speaker_audio(self, speakers_dict, sample_rate, original_file_path):\n",
    "        \"\"\"Save separate audio files for each speaker\"\"\"\n",
    "        try:\n",
    "            # Create output directory based on original file name\n",
    "            base_name = os.path.basename(original_file_path)\n",
    "            file_name = os.path.splitext(base_name)[0]\n",
    "            output_dir = os.path.dirname(original_file_path)\n",
    "            \n",
    "            # Save agent audio\n",
    "            agent_path = os.path.join(output_dir, f\"{file_name}_agent.wav\")\n",
    "            if len(speakers_dict['agent']) > 0:\n",
    "                sf.write(agent_path, speakers_dict['agent'], sample_rate)\n",
    "                print(f\"Agent audio saved to: {agent_path}\")\n",
    "            else:\n",
    "                print(\"No agent audio segments found\")\n",
    "                agent_path = None\n",
    "                \n",
    "            # Save customer audio\n",
    "            customer_path = os.path.join(output_dir, f\"{file_name}_customer.wav\")\n",
    "            if len(speakers_dict['customer']) > 0:\n",
    "                sf.write(customer_path, speakers_dict['customer'], sample_rate)\n",
    "                print(f\"Customer audio saved to: {customer_path}\")\n",
    "            else:\n",
    "                print(\"No customer audio segments found\")\n",
    "                customer_path = None\n",
    "                \n",
    "            return {\n",
    "                'agent_path': agent_path,\n",
    "                'customer_path': customer_path\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving speaker audio: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return {'agent_path': None, 'customer_path': None}\n",
    "\n",
    "    def extract_features(self, y, sample_rate):\n",
    "        \"\"\"Extract acoustic features from audio segments\"\"\"\n",
    "        if y is None or len(y) == 0:\n",
    "            return {k: 0 for k in ['pitch_mean', 'pitch_std', 'pitch_range', 'energy_mean', 'energy_std',\n",
    "                                   'energy_range', 'tempo', 'pause_ratio', 'speech_rate',\n",
    "                                   'spectral_centroid', 'spectral_rolloff']}\n",
    "\n",
    "        # Use larger hop length for faster processing\n",
    "        hop_length = 512\n",
    "        features = {}\n",
    "        \n",
    "        # Extract pitch information using YIN algorithm\n",
    "        features['pitch'] = librosa.yin(y, fmin=50, fmax=500, hop_length=hop_length)\n",
    "        features['pitch_mean'] = np.mean(features['pitch'])\n",
    "        features['pitch_std'] = np.std(features['pitch'])\n",
    "        features['pitch_range'] = np.max(features['pitch']) - np.min(features['pitch'])\n",
    "\n",
    "        # Extract energy/intensity information\n",
    "        features['rms'] = librosa.feature.rms(y=y, hop_length=hop_length)[0]\n",
    "        features['energy_mean'] = np.mean(features['rms'])\n",
    "        features['energy_std'] = np.std(features['rms'])\n",
    "        features['energy_range'] = np.max(features['rms']) - np.min(features['rms'])\n",
    "\n",
    "        # Extract rhythm information\n",
    "        tempo, _ = librosa.beat.beat_track(y=y, sr=sample_rate, hop_length=hop_length)\n",
    "        features['tempo'] = tempo\n",
    "\n",
    "        # Extract pause and speech rate information\n",
    "        non_silent_intervals = librosa.effects.split(y, top_db=20)\n",
    "        features['pause_ratio'] = 1 - (sum(i[1]-i[0] for i in non_silent_intervals) / len(y))\n",
    "        features['speech_rate'] = len(non_silent_intervals) / (len(y) / sample_rate)\n",
    "\n",
    "        # Extract spectral information\n",
    "        features['spectral_centroid'] = np.mean(librosa.feature.spectral_centroid(y=y, sr=sample_rate, hop_length=hop_length)[0])\n",
    "        features['spectral_rolloff'] = np.mean(librosa.feature.spectral_rolloff(y=y, sr=sample_rate, hop_length=hop_length)[0])\n",
    "\n",
    "        return features\n",
    "\n",
    "    def get_llm_analysis(self, agent_features, customer_features):\n",
    "        \"\"\"Get analysis from LLM APIs\"\"\"\n",
    "        llm_results = {}\n",
    "        feature_description = f\"\"\"\n",
    "        Agent audio features:\n",
    "        - Pitch (mean): {agent_features['pitch_mean']:.2f} Hz\n",
    "        - Pitch variation: {agent_features['pitch_std']:.2f} Hz\n",
    "        - Energy level: {agent_features['energy_mean']:.4f}\n",
    "        - Speech rate: {agent_features['speech_rate']:.2f} segments/sec\n",
    "        - Pause ratio: {agent_features['pause_ratio']:.2f}\n",
    "        - Voice quality (spectral centroid): {agent_features['spectral_centroid']:.2f}\n",
    "        Customer audio features:\n",
    "        - Pitch (mean): {customer_features['pitch_mean']:.2f} Hz\n",
    "        - Pitch variation: {customer_features['pitch_std']:.2f} Hz\n",
    "        - Energy level: {customer_features['energy_mean']:.4f}\n",
    "        - Speech rate: {customer_features['speech_rate']:.2f} segments/sec\n",
    "        - Pause ratio: {customer_features['pause_ratio']:.2f}\n",
    "        - Voice quality (spectral centroid): {customer_features['spectral_centroid']:.2f}\n",
    "\"\"\"\n",
    "        prompt = f\"{feature_description}\\n\\nBased on these audio features, please provide:\\n1. Customer sentiment score (0-10)\\n2. Agent politeness score (0-10)\\n3. Agent empathy score (0-10)\\nInclude a brief explanation for each score.\"\n",
    "        \n",
    "        # Try OpenAI analysis \n",
    "        try:\n",
    "            if openai.api_key:\n",
    "                response = openai.chat.completions.create(\n",
    "                    model=\"gpt-4\",\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are an expert in analyzing voice features to determine emotional states and communication quality.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                        ]\n",
    "                        )\n",
    "                llm_results['openai'] = response.choices[0].message.content\n",
    "            else:\n",
    "                llm_results['openai'] = \"OpenAI API key not configured. Please set the OPENAI_API_KEY environment variable.\"\n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            llm_results['openai'] = f\"Error with OpenAI analysis: {error_msg}\"\n",
    "            print(f\"OpenAI API error: {error_msg}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "        # Try Gemini analysis\n",
    "        try:\n",
    "            if os.getenv(\"GOOGLE_API_KEY\"):\n",
    "                model = genai.GenerativeModel('gemini-pro')\n",
    "                response = model.generate_content(prompt)\n",
    "                llm_results['gemini'] = response.text\n",
    "            else:\n",
    "                llm_results['gemini'] = \"Google API key not configured. Please set the GOOGLE_API_KEY environment variable.\"\n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            llm_results['gemini'] = f\"Error with Gemini analysis: {error_msg}\"\n",
    "            print(f\"Gemini API error: {error_msg}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "        return llm_results\n",
    "\n",
    "    def analyze_call(self, audio_path, scores_json_path=None):\n",
    "        \"\"\"Main function to analyze a call recording\"\"\"\n",
    "        y, sample_rate = self.process_audio(audio_path)\n",
    "        if y is None:\n",
    "            return {\n",
    "                'file_name': os.path.basename(audio_path),\n",
    "                'error': \"Failed to process audio file\"\n",
    "            }\n",
    "\n",
    "        # Perform speaker diarization\n",
    "        print(f\"Diarizing speakers for {os.path.basename(audio_path)}...\")\n",
    "        speakers = self.diarize_speakers(y, sample_rate)\n",
    "        \n",
    "        # Save the separated audio files\n",
    "        audio_paths = self.save_speaker_audio(speakers, sample_rate, audio_path)\n",
    "        \n",
    "        # Extract features for each speaker\n",
    "        print(\"Extracting features for each speaker...\")\n",
    "        agent_features = self.extract_features(speakers['agent'], sample_rate)\n",
    "        customer_features = self.extract_features(speakers['customer'], sample_rate)\n",
    "\n",
    "        # Try to load scores from JSON if available\n",
    "        scores = None\n",
    "        if scores_json_path and os.path.exists(scores_json_path):\n",
    "            try:\n",
    "                with open(scores_json_path, 'r') as f:\n",
    "                    scores = json.load(f)\n",
    "                print(f\"Loaded scores from {scores_json_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading scores from {scores_json_path}: {e}\")\n",
    "        \n",
    "        if scores:\n",
    "            # Use the scores from the first cell if available\n",
    "            customer_sentiment = scores.get(\"Customer Sentiment Score\", 0)\n",
    "            agent_politeness = scores.get(\"Agent Politeness Score\", 0)\n",
    "            agent_empathy = scores.get(\"Agent Empathy Score\", 0)\n",
    "        else:\n",
    "            # As a fallback, calculate scores directly\n",
    "            agent_scores = self.calculate_scores(agent_features, 'agent')\n",
    "            customer_scores = self.calculate_scores(customer_features, 'customer')\n",
    "            customer_sentiment = customer_scores.get('sentiment', 0)\n",
    "            agent_politeness = agent_scores.get('politeness', 0)\n",
    "            agent_empathy = agent_scores.get('empathy', 0)\n",
    "\n",
    "        # Get analysis from LLMs\n",
    "        print(\"Getting LLM analysis...\")\n",
    "        llm_analysis = self.get_llm_analysis(agent_features, customer_features)\n",
    "\n",
    "        results = {\n",
    "            'file_name': os.path.basename(audio_path),\n",
    "            'customer_sentiment': customer_sentiment,\n",
    "            'agent_politeness': agent_politeness,\n",
    "            'agent_empathy': agent_empathy,\n",
    "            'agent_features': agent_features,\n",
    "            'customer_features': customer_features,\n",
    "            'llm_analysis': llm_analysis,\n",
    "            'audio_paths': audio_paths\n",
    "        }\n",
    "\n",
    "        return results\n",
    "\n",
    "    def calculate_scores(self, features, speaker_type):\n",
    "        \"\"\"Original score calculation as fallback\"\"\"\n",
    "        scores = {}\n",
    "\n",
    "        if speaker_type == 'customer':\n",
    "            scores['sentiment'] = np.clip(5 +\n",
    "                                         features['pitch_range'] / 50 +\n",
    "                                         features['energy_mean'] * 20 - \n",
    "                                         features['pause_ratio'] * 5, 0, 10)\n",
    "\n",
    "        if speaker_type == 'agent':\n",
    "            scores['politeness'] = np.clip(7 - \n",
    "                                          abs(features['tempo'] - 100) / 20 - \n",
    "                                          features['pitch_std'] / 10 + \n",
    "                                          features['pause_ratio'] * 5, 0, 10)\n",
    "\n",
    "            scores['empathy'] = np.clip(5 + \n",
    "                                       features['pitch_range'] / 40 + \n",
    "                                       features['energy_range'] * 10, 0, 10)\n",
    "\n",
    "        return scores\n",
    "# endregion\n",
    "\n",
    "def save_features_to_json(features, output_path='extracted_features.json'):\n",
    "    \"\"\"Save features to a JSON file\"\"\"\n",
    "    serializable_features = {}\n",
    "    for key, df in features.items():\n",
    "        serializable_features[key] = df.to_dict(orient='records')[0]\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(serializable_features, f, indent=4)\n",
    "    print(f\"Features saved to {output_path}\")\n",
    "\n",
    "def format_features_for_display(features_dict):\n",
    "    \"\"\"Format features dictionary into a readable string\"\"\"\n",
    "    result = []\n",
    "    \n",
    "    # Handle OpenSMILE features\n",
    "    if 'opensmile' in features_dict:\n",
    "        result.append(\"### OpenSMILE Features\")\n",
    "        for key, value in features_dict['opensmile'].iloc[0].items():\n",
    "            result.append(f\"- **{key}**: {value:.4f}\")\n",
    "    \n",
    "    # Handle Praat features\n",
    "    if 'praat' in features_dict:\n",
    "        result.append(\"\\n### Praat Features\")\n",
    "        for key, value in features_dict['praat'].iloc[0].items():\n",
    "            result.append(f\"- **{key}**: {value:.4f}\")\n",
    "    \n",
    "    return \"\\n\".join(result)\n",
    "\n",
    "def format_call_features(features_dict):\n",
    "    \"\"\"Format call analysis features into a readable string\"\"\"\n",
    "    if not features_dict:\n",
    "        return \"No features available\"\n",
    "        \n",
    "    result = []\n",
    "    # Skip 'pitch' and 'rms' which are arrays\n",
    "    skip_keys = ['pitch', 'rms']\n",
    "    \n",
    "    for key, value in features_dict.items():\n",
    "        if key not in skip_keys:\n",
    "            if isinstance(value, (int, float)):\n",
    "                result.append(f\"- **{key.replace('_', ' ').title()}**: {value:.4f}\")\n",
    "            else:\n",
    "                result.append(f\"- **{key.replace('_', ' ').title()}**: {value}\")\n",
    "    \n",
    "    return \"\\n\".join(result)\n",
    "\n",
    "def create_feature_table(features, call_results):\n",
    "    \"\"\"Create a DataFrame with all features for display\"\"\"\n",
    "    # Combine all features into a single dictionary\n",
    "    all_features = {}\n",
    "    \n",
    "    # Add OpenSMILE features\n",
    "    if 'opensmile' in features:\n",
    "        for col in features['opensmile'].columns:\n",
    "            all_features[f\"OpenSMILE: {col}\"] = features['opensmile'][col].values[0]\n",
    "    \n",
    "    # Add Praat features\n",
    "    if 'praat' in features:\n",
    "        for col in features['praat'].columns:\n",
    "            all_features[f\"Praat: {col}\"] = features['praat'][col].values[0]\n",
    "    \n",
    "    # Add agent features\n",
    "    if 'agent_features' in call_results:\n",
    "        for key, value in call_results['agent_features'].items():\n",
    "            if key not in ['pitch', 'rms'] and isinstance(value, (int, float)):\n",
    "                all_features[f\"Agent: {key}\"] = value\n",
    "    \n",
    "    # Add customer features\n",
    "    if 'customer_features' in call_results:\n",
    "        for key, value in call_results['customer_features'].items():\n",
    "            if key not in ['pitch', 'rms'] and isinstance(value, (int, float)):\n",
    "                all_features[f\"Customer: {key}\"] = value\n",
    "    \n",
    "    # Convert to DataFrame for display\n",
    "    df = pd.DataFrame([all_features])\n",
    "    return df\n",
    "\n",
    "def create_analysis_visualization(output_dir, filename, voice_results, call_results):\n",
    "    \"\"\"Create a custom visualization and save it to a file\"\"\"\n",
    "    try:\n",
    "        # Create the output path\n",
    "        output_path = os.path.join(output_dir, f\"{filename}_analysis.png\")\n",
    "\n",
    "        # Create a new figure with a smaller DPI for faster rendering\n",
    "        plt.figure(figsize=(12, 10), dpi=80)\n",
    "\n",
    "        # Get scores\n",
    "        customer_sentiment = voice_results['sentiment']['score']\n",
    "        agent_politeness = voice_results['politeness']['score']\n",
    "        agent_empathy = voice_results['empathy']['score']\n",
    "\n",
    "        # Plot 1: Scores\n",
    "        plt.subplot(221)\n",
    "        metrics = ['Customer Sentiment', 'Agent Politeness', 'Agent Empathy']\n",
    "        values = [float(customer_sentiment), float(agent_politeness), float(agent_empathy)]\n",
    "        bars = plt.bar(metrics, values, color=['blue', 'green', 'purple'])\n",
    "        plt.ylim(0, 10)\n",
    "        plt.title('Call Analysis Scores')\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                     f'{height:.1f}',\n",
    "                     ha='center', va='bottom')\n",
    "\n",
    "        # Plot 2: Voice Feature Comparison\n",
    "        plt.subplot(222)\n",
    "        if 'agent_features' in call_results and 'customer_features' in call_results:\n",
    "            features_to_plot = ['pitch_mean', 'energy_mean', 'speech_rate']\n",
    "\n",
    "            # Get values, handling missing features\n",
    "            agent_values = [call_results['agent_features'].get(f, 0) for f in features_to_plot]\n",
    "            customer_values = [call_results['customer_features'].get(f, 0) for f in features_to_plot]\n",
    "\n",
    "            # Normalize for better visualization\n",
    "            max_vals = [max(a, c) if max(a,c) > 0 else 1 for a, c in zip(agent_values, customer_values)]\n",
    "            agent_norm = [a/m*5 for a, m in zip(agent_values, max_vals)]\n",
    "            customer_norm = [c/m*5 for c, m in zip(customer_values, max_vals)]\n",
    "\n",
    "            x = np.arange(len(features_to_plot))\n",
    "            width = 0.35\n",
    "\n",
    "            plt.bar(x - width/2, agent_norm, width, label='Agent')\n",
    "            plt.bar(x + width/2, customer_norm, width, label='Customer')\n",
    "            plt.xticks(x, [f.replace('_', ' ').title() for f in features_to_plot], rotation=45, ha=\"right\")\n",
    "            plt.ylabel('Normalized Value')\n",
    "            plt.legend()\n",
    "            plt.title('Normalized Voice Features')\n",
    "\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, 'Feature data not available',horizontalalignment='center', verticalalignment='center')\n",
    "            plt.title('Voice Feature Comparison')\n",
    "\n",
    "        # Plot 3: Additional Features\n",
    "        plt.subplot(223)\n",
    "        if 'agent_features' in call_results:\n",
    "            try:\n",
    "                features_to_plot = ['pause_ratio', 'spectral_centroid', 'tempo']\n",
    "                \n",
    "                # Extract values safely, ensuring they are simple numbers\n",
    "                values = []\n",
    "                for f in features_to_plot:\n",
    "                    val = call_results['agent_features'].get(f, 0)\n",
    "                    # Convert any complex objects to float\n",
    "                    if isinstance(val, (list, np.ndarray)):\n",
    "                        val = float(val[0]) if len(val) > 0 else 0.0\n",
    "                    values.append(float(val))\n",
    "                \n",
    "                # Normalize for better visualization\n",
    "                max_val = max(values) if max(values) > 0 else 1.0\n",
    "                norm_values = [float(v/max_val*5) for v in values]\n",
    "                \n",
    "                # Use integers for x-axis\n",
    "                x_pos = np.arange(len(features_to_plot))\n",
    "                \n",
    "                # Create a simple bar chart with explicit numeric x-positions\n",
    "                for i, (pos, val, name) in enumerate(zip(x_pos, norm_values, features_to_plot)):\n",
    "                    plt.bar(pos, val, color='teal')\n",
    "                    \n",
    "                plt.xticks(x_pos, [f.replace('_', ' ').title() for f in features_to_plot], \n",
    "                           rotation=45, ha=\"right\")\n",
    "                plt.title('Agent Voice Characteristics (Normalized)')\n",
    "                plt.ylabel('Normalized Value')\n",
    "            except Exception as e:\n",
    "                print(f\"Error in Plot 3: {str(e)}\")\n",
    "                plt.text(0.5, 0.5, f'Error creating plot: {str(e)}',\n",
    "                         horizontalalignment='center')\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, 'Feature data not available',\n",
    "                     horizontalalignment='center', verticalalignment='center')\n",
    "            plt.title('Agent Voice Characteristics')\n",
    "\n",
    "        # Plot 4: LLM Analysis Summary\n",
    "        plt.subplot(212)\n",
    "        plt.axis('off')\n",
    "\n",
    "        llm_text = \"LLM Analysis Summary:\\n\\n\"\n",
    "        if 'llm_analysis' in call_results:\n",
    "            for llm, analysis in call_results['llm_analysis'].items():\n",
    "                if analysis:\n",
    "                    # Check if the analysis contains an error message\n",
    "                    if analysis.startswith(\"Error with\") or \"API key not configured\" in analysis:\n",
    "                        llm_text += f\"{llm.upper()}: {analysis}\\n\\n\"\n",
    "                    else:\n",
    "                        # Limit the text length for display\n",
    "                        summary = analysis[:300] + \"...\" if len(analysis) > 300 else analysis\n",
    "                        llm_text += f\"{llm.upper()}:\\n{summary}\\n\\n\"\n",
    "                else:\n",
    "                    llm_text += f\"{llm.upper()}: No response\\n\\n\"\n",
    "        else:\n",
    "            llm_text += \"No LLM analysis available\"\n",
    "\n",
    "        plt.text(0.05, 0.95, llm_text, fontsize=9,\n",
    "                 verticalalignment='top', wrap=True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_path, dpi=80)  # Lower DPI for faster saving\n",
    "        plt.close()\n",
    "\n",
    "        return output_path\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating visualization: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        # Return a fallback path even if the visualization fails\n",
    "        return os.path.join(output_dir, f\"{filename}_analysis.png\")\n",
    "\n",
    "def create_waveform_comparison(original_audio, noise_reduced_audio, sample_rate, output_path):\n",
    "    \"\"\"Create a waveform comparison between original and noise-reduced audio\"\"\"\n",
    "    try:\n",
    "        # Use a smaller figure size and lower DPI for faster rendering\n",
    "        plt.figure(figsize=(10, 6), dpi=80)\n",
    "        \n",
    "        # Plot original audio\n",
    "        plt.subplot(211)\n",
    "        plt.plot(np.linspace(0, len(original_audio)/sample_rate, len(original_audio)), original_audio)\n",
    "        plt.title('Original Audio Waveform')\n",
    "        plt.xlabel('Time (s)')\n",
    "        plt.ylabel('Amplitude')\n",
    "        \n",
    "        # Plot noise-reduced audio\n",
    "        plt.subplot(212)\n",
    "        plt.plot(np.linspace(0, len(noise_reduced_audio)/sample_rate, len(noise_reduced_audio)), noise_reduced_audio, color='green')\n",
    "        plt.title('Noise-Reduced Audio Waveform')\n",
    "        plt.xlabel('Time (s)')\n",
    "        plt.ylabel('Amplitude')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_path, dpi=80)  # Lower DPI for faster saving\n",
    "        plt.close()\n",
    "        \n",
    "        return output_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating waveform comparison: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to process audio in a separate thread\n",
    "def process_audio_thread(audio_path, progress=None):\n",
    "    \"\"\"Process audio in a separate thread to avoid blocking the UI\"\"\"\n",
    "    try:\n",
    "        # Initialize components\n",
    "        audio_processor = AudioProcessor()\n",
    "        \n",
    "        # Process the audio file\n",
    "        print(f\"Processing audio file: {audio_path}\")\n",
    "        if progress:\n",
    "            progress(0.1, \"Loading audio file...\")\n",
    "        processed_file, y_reduced, sample_rate, noise_reduced_path = audio_processor.process_audio(audio_path)\n",
    "        \n",
    "        if not processed_file:\n",
    "            return None, None, None, None\n",
    "        \n",
    "        if progress:\n",
    "            progress(0.3, \"Audio processing complete\")\n",
    "        \n",
    "        # Load original audio for comparison\n",
    "        y_original, sr_original = librosa.load(audio_path, sr=None)\n",
    "        \n",
    "        # Create waveform comparison\n",
    "        if progress:\n",
    "            progress(0.4, \"Creating waveform comparison...\")\n",
    "        waveform_comparison_path = os.path.join(os.path.dirname(processed_file), \n",
    "                                               f\"{os.path.basename(audio_path)}_waveform_comparison.png\")\n",
    "        create_waveform_comparison(y_original, y_reduced, sample_rate, waveform_comparison_path)\n",
    "        \n",
    "        return processed_file, y_reduced, sample_rate, noise_reduced_path, waveform_comparison_path\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        print(f\"Error in audio processing thread: {e}\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "def process_and_analyze(audio_path, progress=gr.Progress()):\n",
    "    \"\"\"Process and analyze an audio file given its path\"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(audio_path):\n",
    "            return \"Error: Audio file not found\", None, None, None, None, None, None\n",
    "        \n",
    "        # Process audio in a separate thread to avoid blocking the UI\n",
    "        progress(0.05, \"Starting audio processing...\")\n",
    "        processed_file, y_reduced, sample_rate, noise_reduced_path, waveform_comparison_path = process_audio_thread(\n",
    "            audio_path, \n",
    "            lambda p, msg: progress(p, msg)\n",
    "        )\n",
    "        \n",
    "        if not processed_file:\n",
    "            return \"Audio processing failed\", None, None, None, None, None, None\n",
    "        \n",
    "        # Initialize components for analysis\n",
    "        feature_extractor = FeatureExtractor()\n",
    "        voice_analyzer = VoiceAnalyzer()\n",
    "        call_analyzer = CallAnalyzer()\n",
    "        \n",
    "        # Extract features\n",
    "        progress(0.5, \"Extracting audio features...\")\n",
    "        features = feature_extractor.extract_all_features(processed_file, y_reduced, sample_rate)\n",
    "        \n",
    "        # Save features to JSON for reference\n",
    "        features_json_path = f\"{os.path.splitext(processed_file)[0]}_features.json\"\n",
    "        save_features_to_json(features, features_json_path)\n",
    "        \n",
    "        # Analyze voice\n",
    "        progress(0.6, \"Analyzing voice characteristics...\")\n",
    "        voice_results, indicators = voice_analyzer.analyze(features)\n",
    "        \n",
    "        # Save scores to JSON\n",
    "        # Save scores to JSON\n",
    "        scores_json_path = f\"{os.path.splitext(processed_file)[0]}_scores.json\"\n",
    "        # FIX: Wrap values in float() to convert from numpy types\n",
    "        score_output = {  \n",
    "            \"Customer Sentiment Score\": float(voice_results['sentiment']['score']),\n",
    "            \"Agent Politeness Score\": float(voice_results['politeness']['score']),\n",
    "            \"Agent Empathy Score\": float(voice_results['empathy']['score'])\n",
    "        }\n",
    "        with open(scores_json_path, \"w\") as f:\n",
    "            json.dump(score_output, f, indent=4)\n",
    "        \n",
    "        # Run call analyzer with speaker diarization\n",
    "        progress(0.7, \"Running call analysis with speaker diarization...\")\n",
    "        call_results = call_analyzer.analyze_call(processed_file, scores_json_path)\n",
    "        \n",
    "        # Get the paths to the separated audio files\n",
    "        agent_audio_path = call_results.get('audio_paths', {}).get('agent_path', None)\n",
    "        customer_audio_path = call_results.get('audio_paths', {}).get('customer_path', None)\n",
    "        \n",
    "        # Create visualization\n",
    "        progress(0.9, \"Creating visualization...\")\n",
    "        analysis_image_path = create_analysis_visualization(\n",
    "            os.path.dirname(processed_file),\n",
    "            os.path.basename(audio_path),\n",
    "            voice_results,\n",
    "            call_results\n",
    "        )\n",
    "        \n",
    "        # Format all features for display\n",
    "        formatted_features = format_features_for_display(features)\n",
    "        \n",
    "        # Format agent and customer features\n",
    "        agent_features_formatted = format_call_features(call_results.get('agent_features', {}))\n",
    "        customer_features_formatted = format_call_features(call_results.get('customer_features', {}))\n",
    "        \n",
    "        # Prepare results text with all features\n",
    "        results_text = f\"\"\"\n",
    "## Analysis Results for {os.path.basename(audio_path)}\n",
    "\n",
    "**Duration:** {librosa.get_duration(y=y_reduced, sr=sample_rate):.2f} seconds\n",
    "\n",
    "### Scores\n",
    "- **Customer Sentiment:** {voice_results['sentiment']['score']:.2f}/10 - {voice_results['sentiment']['description']}\n",
    "- **Agent Politeness:** {voice_results['politeness']['score']:.2f}/10 - {voice_results['politeness']['description']}\n",
    "- **Agent Empathy:** {voice_results['empathy']['score']:.2f}/10 - {voice_results['empathy']['description']}\n",
    "\n",
    "### Noise Reduction\n",
    "Noise reduction has been applied to improve audio quality. The noise-reduced audio is available for playback.\n",
    "\n",
    "### Agent Features\n",
    "{agent_features_formatted}\n",
    "\n",
    "### Customer Features\n",
    "{customer_features_formatted}\n",
    "\n",
    "## Extracted Audio Features\n",
    "{formatted_features}\n",
    "\"\"\"\n",
    "        \n",
    "        # Create a feature table for display\n",
    "        feature_table = create_feature_table(features, call_results)\n",
    "        \n",
    "        progress(1.0, \"Analysis complete!\")\n",
    "        return results_text, noise_reduced_path, agent_audio_path, customer_audio_path, analysis_image_path, waveform_comparison_path, feature_table\n",
    "        \n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        error_msg = str(e)\n",
    "        # Truncate error message to avoid content length issues\n",
    "        if len(error_msg) > 500:\n",
    "            error_msg = error_msg[:500] + \"...\"\n",
    "        return f\"Error: {error_msg}\", None, None, None, None, None, None\n",
    "\n",
    "# Create a Gradio interface with streamlined components\n",
    "demo = gr.Interface(\n",
    "    fn=process_and_analyze,\n",
    "    inputs=gr.Audio(type=\"filepath\", label=\"Upload Call Recording\"),\n",
    "    outputs=[\n",
    "        gr.Markdown(label=\"Analysis Results\"),\n",
    "        gr.Audio(label=\"Noise-Reduced Audio\"),\n",
    "        gr.Audio(label=\"Agent Audio\"),\n",
    "        gr.Audio(label=\"Customer Audio\"),\n",
    "        gr.Image(label=\"Analysis Visualization\"),\n",
    "        gr.Image(label=\"Waveform Comparison (Original vs. Noise-Reduced)\"),\n",
    "        gr.Dataframe(label=\"All Extracted Features\")\n",
    "    ],\n",
    "    title=\"Call Audio Analysis Tool\",\n",
    "    description=\"\"\"\n",
    "    Upload a call recording to analyze:\n",
    "    1. Noise reduction to clean the audio\n",
    "    2. Customer sentiment analysis\n",
    "    3. Agent politeness and empathy evaluation\n",
    "    4. Speaker diarization with language detection\n",
    "    \"\"\",\n",
    "    theme=gr.themes.Soft(primary_hue=\"indigo\", secondary_hue=\"violet\"),\n",
    "    examples=[],  # You can add example audio files here if needed\n",
    "    cache_examples=False\n",
    ")\n",
    "\n",
    "# Launch the app with optimized settings\n",
    "if __name__ == \"__main__\":\n",
    "   \n",
    "    demo.launch(share=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
